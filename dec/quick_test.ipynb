{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import dgl\n",
    "import torch\n",
    "from torch import nn\n",
    "import networkx as nx\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import src.dag_utils as dagu\n",
    "import src.utils as utils\n",
    "from src.arch import FB_DAGConv, ParallelMLPSum, SharedMLPSum\n",
    "from src.models import Model, LinDAGRegModel\n",
    "from src.baselines_archs import GAT, MLP, MyGCNN, GraphSAGE, GIN\n",
    "\n",
    "import os\n",
    "\n",
    "# Ser random seed\n",
    "SEED = 10\n",
    "PATH = 'results/diffusion/'\n",
    "SAVE = False\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "dgl.random.seed(SEED)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "# torch.cuda.set_per_process_memory_fraction(.5, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 2000\n",
    "\n",
    "data_p = {\n",
    "    'n_tries': 1,  #25,\n",
    "\n",
    "    ## Graph parameters\n",
    "    'p': 0.2,  # .2                  # Edge prob in Erdos-Renyi DAG\n",
    "    'N': 100,                    # Number of nodes\n",
    "\n",
    "    ## Signal parameters\n",
    "    'M': M,                   # Number of observed signals\n",
    "    'M_train': int(0.7 * M),  # Samples selected for training\n",
    "    'M_val': int(0.2 * M),    # Samples selected for validation\n",
    "    'M_test': int(0.1 * M),   # Samples selected for test\n",
    "    'src_t': 'constant',          # 'random' or 'constant'\n",
    "    'max_src_node': 25, #25,           # Maximum index of nodes allowed to be sources\n",
    "    'n_sources': 5,             # Maximum Number of source nodes\n",
    "    'n_p_x': .05,\n",
    "    'n_p_y': .05,                 # Normalized noise power\n",
    "    'max_GSO': 100,              # Maximum index of GSOs involved in the diffusion\n",
    "    'min_GSO': 50,               # Minimum index of GSOs involved in the diffusion\n",
    "    'n_GSOs': 25                 # Number of GSOs\n",
    "}\n",
    "\n",
    "# Model parameters\n",
    "default_arch_args = {\n",
    "    'in_dim': 1,        # Input dimension\n",
    "    'hid_dim': 32,     # Hidden dimension\n",
    "    'out_dim': 1,       # Output dimension\n",
    "    'n_layers': 2,#2,  # 3 also works well          # Number of layers\n",
    "    'l_act': None,\n",
    "    'bias': True,\n",
    "}\n",
    "\n",
    "default_mod_p = {\n",
    "    'bs': 25,           # Size of the batch\n",
    "    'lr': 5e-4,         # Learning rate\n",
    "    'epochs': 50,  #50,       # Number of training epochs \n",
    "    'pat': 25,  # 15        # Number of non-decreasing epoch to stop training\n",
    "    'wd': 1e-4,         # Weight decay\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_signals(d_p, GSOs):\n",
    "    range_GSO = np.arange(d_p['min_GSO'], d_p['max_GSO'])\n",
    "    gsos_idx = np.random.choice(range_GSO, size=d_p['n_GSOs'], replace=False)\n",
    "    sel_GSOs = GSOs[gsos_idx]\n",
    "    Yn_t, X_t, Y_t = dagu.create_diff_data(d_p['M'], sel_GSOs, d_p['max_src_node'], d_p['n_p_x'], d_p['n_p_y'],\n",
    "                                           d_p['n_sources'], src_t=d_p['src_t'], torch_tensor=True, verb=False)\n",
    "    \n",
    "    X_data = {'train': X_t[:d_p['M_train']], 'val': X_t[d_p['M_train']:-d_p['M_test']], 'test': X_t[-d_p['M_test']:]}\n",
    "    Y_data = {'train': Yn_t[:d_p['M_train']], 'val': Yn_t[d_p['M_train']:-d_p['M_test']],\n",
    "              'test': Y_t[-d_p['M_test']:]}\n",
    "        \n",
    "    return X_data, Y_data, sel_GSOs, gsos_idx\n",
    "\n",
    "def run_exp(d_p, d_arc_args, d_mod_p, exps, verb=True):\n",
    "    # Create error variables\n",
    "    err = np.zeros((d_p['n_tries'], len(exps)))\n",
    "    std = np.zeros((d_p['n_tries'], len(exps)))\n",
    "    times = np.zeros((d_p['n_tries'], len(exps)))\n",
    "\n",
    "    t_begin = time.time()\n",
    "    # for i in range(d_p['n_tries']):\n",
    "    with tqdm(total=d_p['n_tries']*len(exps), disable=False) as pbar:\n",
    "        for i in range(d_p['n_tries']):\n",
    "            Adj, W, GSOs, Psi = utils.get_graph_data(d_p, get_Psi=True)\n",
    "            X_data, Y_data, sel_GSOs, sel_GSOs_idx = get_signals(d_p, GSOs)\n",
    "            \n",
    "            for j, exp in enumerate(exps):\n",
    "                # Combine default and experiment parameters    \n",
    "                arc_p = {**exp['arc_p']}\n",
    "                arc_p['args'] = {**d_arc_args, **arc_p['args']} if 'args' in arc_p.keys() else {**d_arc_args}\n",
    "                mod_p = {**d_mod_p, **exp['mod_p']} if 'mod_p' in exp.keys() else d_mod_p\n",
    "\n",
    "                # Fit and test nonlinear models\n",
    "                GSO = utils.select_GSO(arc_p, GSOs, sel_GSOs, W, Adj)\n",
    "                K = GSO.shape[0] if isinstance(GSO, torch.Tensor) and len(GSO.shape) == 3 else 0  \n",
    "                arch = utils.instantiate_arch(arc_p, K)                \n",
    "                model = Model(arch, device=device)\n",
    "\n",
    "\n",
    "                params = arch.n_params if hasattr(arch, 'n_params') else None \n",
    "                print(f'-{i}. {exp[\"leg\"]}: n_params: {params}')\n",
    "\n",
    "                continue \n",
    "\n",
    "\n",
    "                t_i = time.time()\n",
    "                model.fit(X_data, Y_data, GSO, mod_p['lr'], mod_p['epochs'], mod_p['bs'], mod_p['wd'], patience=mod_p['pat'])\n",
    "                t_e = time.time() - t_i\n",
    "                err[i,j], std[i,j] = model.test(X_data['test'], Y_data['test'], GSO, exp['arc_p']['arch'])\n",
    "\n",
    "\n",
    "            # times[i,j] = t_e\n",
    "\n",
    "            params = arch.n_params if hasattr(arch, 'n_params') else None \n",
    "                \n",
    "            # Progress\n",
    "            pbar.update(1)\n",
    "            if verb:\n",
    "                print(f'-{i}. {exp[\"leg\"]}: err: {err[i,j]:.3f} | std: {std[i,j]:.3f}  |' +\n",
    "                      f' time: {times[i,j]:.1f} | n_params: {params}')\n",
    "\n",
    "    total_t = (time.time() - t_begin)/60\n",
    "    print(f'----- Ellapsed time: {total_t:.2f} minutes -----')\n",
    "    return err, std, times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3702fb2bc4c4a8284958e1c8addb10e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0. PDCN-20: err: 0.000 | std: 0.000  | time: 0.0 | n_params: 23060\n",
      "----- Ellapsed time: 0.00 minutes -----\n"
     ]
    }
   ],
   "source": [
    "mod_p_init = default_mod_p.copy()\n",
    "mod_p_init['pat'] = 50\n",
    "\n",
    "verb = True\n",
    "\n",
    "# Experiments to be run\n",
    "Exps = [\n",
    "    # Our Models\n",
    "    {'arc_p': {'arch': FB_DAGConv, 'GSO': 'rnd_GSOs', 'n_gsos': 5}, 'leg': 'DCN-5'},\n",
    "    \n",
    "    {'arc_p': {'arch': ParallelMLPSum, 'input_dim': 1, 'hidden_dims': [32]*2, 'output_dim': 1, 'GSO': 'rnd_GSOs', 'n_inputs': 5, 'n_gsos': 5}, 'leg': 'PDCN-5'},\n",
    "\n",
    "    {'arc_p': {'arch': SharedMLPSum, 'input_dim': 1, 'hidden_dims': [32]*2, 'output_dim': 1, 'GSO': 'rnd_GSOs', 'n_inputs': 5, 'n_gsos': 5}, 'leg': 'SDCN-5'},\n",
    "\n",
    "    {'arc_p': {'arch': FB_DAGConv, 'GSO': 'rnd_GSOs', 'n_gsos': 5}, 'leg': 'DCN-5'},\n",
    "\n",
    "    {'arc_p': {'arch': SharedMLPSum, 'input_dim': 1, 'hidden_dims': [32]*2, 'output_dim': 1, 'GSO': 'rnd_GSOs', 'n_inputs': 20, 'n_gsos': 20}, 'leg': 'SDCN-20'},\n",
    "\n",
    "    {'arc_p': {'arch': ParallelMLPSum, 'input_dim': 1, 'hidden_dims': [32]*2, 'output_dim': 1, 'GSO': 'rnd_GSOs', 'n_inputs': 20, 'n_gsos': 20}, 'leg': 'PDCN-20'},\n",
    "\n",
    "]\n",
    "\n",
    "err, std, times = run_exp(data_p, default_arch_args, mod_p_init, Exps, verb=verb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
