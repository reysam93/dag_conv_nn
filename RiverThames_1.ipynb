{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awciKO9BV-6W"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall torch==2.6.0 torchaudio torchvision torch-scatter torch-sparse torch-cluster torch-spline-conv -y\n"
      ],
      "metadata": {
        "id": "ZygYL6k7WCHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall torch torchaudio torchvision torch-scatter torch-sparse torch-cluster torch-spline-conv -y\n",
        "!pip install torch==2.4.0+cu121  --index-url https://download.pytorch.org/whl/cu121\n"
      ],
      "metadata": {
        "id": "PNfgVZdxWDsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall dgl -y\n",
        "!pip install dgl -f https://data.dgl.ai/wheels/torch-2.4/cu124/repo.html\n"
      ],
      "metadata": {
        "id": "QgeEhEanWFcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-2.4.0+cu121.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-2.4.0+cu121.html\n",
        "!pip install torch-geometric\n",
        "!pip install igraph"
      ],
      "metadata": {
        "id": "wece0UTkWHSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "from torch import nn\n",
        "import networkx as nx\n",
        "from tqdm.auto import tqdm\n",
        "import sys\n",
        "import os\n",
        "import pickle\n",
        "from numpy import linalg as la\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "\n",
        "\n",
        "import src.dag_utils as dagu\n",
        "import src.utils as utils\n",
        "from src.arch import DAGConv, FB_DAGConv, SF_DAGConv, ADCN , ParallelMLPSum, SharedMLPSum, SMLP\n",
        "from src.models import Model, LinDAGRegModel\n",
        "from src.baselines_archs import GAT, MLP, MyGCNN, GraphSAGE, GIN\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"dgl\")\n",
        "import os\n",
        "import igraph as ig\n",
        "from src.utils_Dagnn import *\n",
        "from src.DAGNN import DAGNN, DVAE\n",
        "\n",
        "\n",
        "\n",
        "# Ser random seed\n",
        "SEED = 10\n",
        "PATH = 'results/diffusion/'\n",
        "SAVE = True\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "# dgl.random.seed(SEED)\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "# torch.cuda.set_per_process_memory_fraction(.5, device=device)\n",
        "\n",
        "\n",
        "\n",
        "M = 343\n",
        "\n",
        "data_p = {\n",
        "    'n_tries': 25,  #25,\n",
        "\n",
        "    ## Graph parameters\n",
        "    'p': 0.2,  # .2                  # Edge prob in Erdos-Renyi DAG\n",
        "    'N': 20,                    # Number of nodes\n",
        "\n",
        "    ## Signal parameters\n",
        "    'M': M,                   # Number of observed signals\n",
        "    'M_train': int(0.7 * M),  # Samples selected for training\n",
        "    'M_val': int(0.2 * M),    # Samples selected for validation\n",
        "    'M_test': int(0.1 * M),   # Samples selected for test\n",
        "    'src_t': 'constant',          # 'random' or 'constant'\n",
        "    'max_src_node': 15, #25,           # Maximum index of nodes allowed to be sources\n",
        "    'n_sources': 14,             # Maximum Number of source nodes\n",
        "    'n_p_x': .05,\n",
        "    'n_p_y': .05,                 # Normalized noise power\n",
        "    'max_GSO': 20,              # Maximum index of GSOs involved in the diffusion\n",
        "    'min_GSO': 0,               # Minimum index of GSOs involved in the diffusion\n",
        "    'n_GSOs': 20,            # Number of GSOs\n",
        "    'concentration': 'sulphate.npy'\n",
        "}\n",
        "\n",
        "default_mod_p = {\n",
        "    'bs': 25,           # Size of the batch\n",
        "    'lr': 5e-4,         # Learning rate\n",
        "    'epochs': 100,  #50,       # Number of training epochs\n",
        "    'pat': 25,  # 15        # Number of non-decreasing epoch to stop training\n",
        "    'wd': 1e-4,         # Weight decay\n",
        "}\n",
        "\n",
        "default_arch_args = {\n",
        "    'in_dim': 1,        # Input dimension\n",
        "    'hid_dim': 32,     # Hidden dimension\n",
        "    'out_dim': 1,       # Output dimension\n",
        "    'n_layers': 2,#2,  # 3 also works well          # Number of layers\n",
        "    'l_act': None,\n",
        "    'bias': True,\n",
        "}\n",
        "\n",
        "\n",
        "def add_noise(signal, n_p):\n",
        "    shape = signal.shape\n",
        "\n",
        "    M = shape[0]\n",
        "    N = shape[1]\n",
        "\n",
        "    if n_p <= 0:\n",
        "        return signal\n",
        "\n",
        "    signal_norm = torch.norm(signal, p=2, dim=1, keepdim=True)\n",
        "    signal_norm[signal_norm == 0] = 1\n",
        "    noise = torch.randn(M, N, 1, device=signal.device)\n",
        "    noise_norm = torch.norm(noise, p=2, dim=1, keepdim=True)\n",
        "    noise = noise * signal_norm * torch.sqrt(torch.tensor(n_p)) / noise_norm\n",
        "\n",
        "    return signal + noise\n",
        "\n",
        "\n",
        "\n",
        "def get_real_data(d_dat_p ,get_Psi=False):\n",
        "\n",
        "    nodes = ['CH','CL','CN','CU','EN','EV','LE','LO','OC','PA','RA','TH','TM','WI','KE','TN','TS','TW','TSO','TR']\n",
        "\n",
        "    Adj = np.load('/content/drive/MyDrive/Thames/' + 'adj_matrix.npy')\n",
        "\n",
        "    W = la.inv(np.eye(d_dat_p['N']) - Adj)\n",
        "    W_inf = la.inv(W)\n",
        "    dag = nx.from_numpy_array(Adj.T, create_using=nx.DiGraph())\n",
        "    if get_Psi:\n",
        "        Psi = np.array([dagu.compute_Dq(dag, i, d_dat_p['N']) for i in range(d_dat_p['N'])]).T\n",
        "        GSOs = np.array([(W * Psi[:,i]) @ W_inf for i in range(d_dat_p['N'])])\n",
        "        return Adj, W, GSOs, Psi\n",
        "    GSOs = np.array([(W * dagu.compute_Dq(dag, i, d_dat_p['N'])) @ W_inf for i in range(d_dat_p['N'])])\n",
        "\n",
        "    return Adj, W, GSOs\n",
        "\n",
        "\n",
        "def is_upper_triangular(adj_matrix):\n",
        "    return np.allclose(adj_matrix, np.triu(adj_matrix))\n",
        "\n",
        "def randomize_dag_adj(A_true, alpha):\n",
        "    num_nodes = A_true.shape[0]\n",
        "\n",
        "    while True:\n",
        "        A_rand = np.triu(A_true.copy())\n",
        "\n",
        "        total_possible_edges = (num_nodes * (num_nodes - 1)) // 2\n",
        "        num_modifications = int(alpha * total_possible_edges)\n",
        "\n",
        "        possible_edges = [(i, j) for i in range(num_nodes) for j in range(i+1, num_nodes)]\n",
        "\n",
        "        modifications = 0\n",
        "        while modifications < num_modifications:\n",
        "            action = random.choice(['add', 'remove'])\n",
        "            i, j = random.choice(possible_edges)\n",
        "\n",
        "            if action == 'add' and A_rand[i, j] == 0:\n",
        "                A_rand[i, j] = 1\n",
        "                modifications += 1\n",
        "            elif action == 'remove' and A_rand[i, j] == 1:\n",
        "                A_rand[i, j] = 0\n",
        "                modifications += 1\n",
        "\n",
        "        if is_upper_triangular(A_rand):\n",
        "            return A_rand\n",
        "\n",
        "\n",
        "def Thames(d_p,GSOs, pollutant=None):\n",
        "\n",
        "    range_GSO = np.arange(d_p['min_GSO'], d_p['max_GSO'])\n",
        "    gsos_idx = np.random.choice(range_GSO, size=d_p['n_GSOs'], replace=False)\n",
        "    sel_GSOs = GSOs[gsos_idx]\n",
        "    t1 = np.load('/content/drive/MyDrive/Thames/' + pollutant)\n",
        "    Yn_t = torch.tensor(t1, dtype = torch.float32)\n",
        "\n",
        "    Xn_t = Yn_t.clone()\n",
        "    Xn_t[:,-6:,:]  = 0\n",
        "\n",
        "    Xn_t = add_noise(Xn_t, 0.00)\n",
        "    Yn_t = add_noise(Yn_t, 0.00)\n",
        "\n",
        "    X_data = {'train': Xn_t[:d_p['M_train']], 'val': Xn_t[d_p['M_train']:-d_p['M_test']], 'test': Xn_t[-d_p['M_test']:]}\n",
        "    Y_data = {'train': Yn_t[:d_p['M_train']], 'val': Yn_t[d_p['M_train']:-d_p['M_test']], 'test': Yn_t[-d_p['M_test']:]}\n",
        "\n",
        "    return X_data, Y_data, sel_GSOs, gsos_idx\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_signals(d_p, GSOs):\n",
        "    range_GSO = np.arange(d_p['min_GSO'], d_p['max_GSO'])\n",
        "    gsos_idx = np.random.choice(range_GSO, size=d_p['n_GSOs'], replace=False)\n",
        "    sel_GSOs = GSOs[gsos_idx]\n",
        "    Yn_t, X_t, Y_t = dagu.create_diff_data(d_p['M'], sel_GSOs, d_p['max_src_node'], d_p['n_p_x'], d_p['n_p_y'],\n",
        "                                           d_p['n_sources'], src_t=d_p['src_t'], torch_tensor=True, verb=False)\n",
        "\n",
        "\n",
        "\n",
        "    X_data = {'train': X_t[:d_p['M_train']], 'val': X_t[d_p['M_train']:-d_p['M_test']], 'test': X_t[-d_p['M_test']:]}\n",
        "    Y_data = {'train': Yn_t[:d_p['M_train']], 'val': Yn_t[d_p['M_train']:-d_p['M_test']],\n",
        "              'test': Y_t[-d_p['M_test']:]}\n",
        "\n",
        "    return X_data, Y_data, sel_GSOs, gsos_idx\n",
        "\n",
        "\n",
        "def run_exp(d_p, d_arc_args, d_mod_p, exps, verb=True):\n",
        "    # Create error variables\n",
        "    print(d_p['concentration'])\n",
        "    err = np.zeros((d_p['n_tries'], len(exps)))\n",
        "    std = np.zeros((d_p['n_tries'], len(exps)))\n",
        "    times = np.zeros((d_p['n_tries'], len(exps)))\n",
        "\n",
        "    t_begin = time.time()\n",
        "    # for i in range(d_p['n_tries']):\n",
        "    with tqdm(total=d_p['n_tries']*len(exps), disable=False) as pbar:\n",
        "        for i in range(d_p['n_tries']):\n",
        "            Adj, W, GSOs, Psi = get_real_data(d_p, get_Psi=True)\n",
        "\n",
        "            X_data, Y_data, sel_GSOs, sel_GSOs_idx = Thames(d_p, GSOs, d_p['concentration'])\n",
        "\n",
        "            for j, exp in enumerate(exps):\n",
        "                arc_p = {**exp['arc_p']}\n",
        "\n",
        "                arc_p['args'] = {**d_arc_args, **arc_p['args']} if 'args' in arc_p.keys() else {**d_arc_args}\n",
        "                mod_p = {**d_mod_p, **exp['mod_p']} if 'mod_p' in exp.keys() else d_mod_p\n",
        "\n",
        "                if exp['arc_p']['arch'] == LinDAGRegModel:\n",
        "                    # Fit and test linear model\n",
        "                    if 'transp' in arc_p.keys() and arc_p['transp']:\n",
        "                        dag_T = nx.from_numpy_array(Adj, create_using=nx.DiGraph())\n",
        "                        Psi = np.array([dagu.compute_Dq(dag_T, i, d_p['N']) for i in range(d_p['N'])]).T\n",
        "                        arc_p['transp'] = False\n",
        "\n",
        "                    Psi_sel = utils.select_GSO(arc_p, Psi.T, Psi[:,sel_GSOs_idx].T, W, Adj, sel_GSOs_idx).numpy().T\n",
        "                    lin_model = LinDAGRegModel(W, Psi_sel)\n",
        "\n",
        "                    t_i = time.time()\n",
        "                    lin_model.fit(X_data['train'], Y_data['train'])\n",
        "                    t_e = time.time() - t_i\n",
        "                    err[i,j], std[i,j] = lin_model.test(X_data['test'], Y_data['test'])\n",
        "                    params = lin_model.n_params if hasattr(lin_model, 'n_params') else None\n",
        "\n",
        "\n",
        "                elif exp['arc_p']['arch'] == DVAE:\n",
        "                    X_data1 = DVAE_exp(Adj, X_data)\n",
        "                    GSO = utils.select_GSO(arc_p, GSOs, sel_GSOs, W, Adj)\n",
        "                    K = GSO.shape[0] if isinstance(GSO, torch.Tensor) and len(GSO.shape) == 3 else 0\n",
        "                    arch = utils.instantiate_arch(arc_p, K)\n",
        "                    model = Model(arch, device=device)\n",
        "\n",
        "                    t_i = time.time()\n",
        "                    model.fit(X_data1, Y_data, GSO, mod_p['lr'], mod_p['epochs'], mod_p['bs'], mod_p['wd'], patience=mod_p['pat'])\n",
        "                    t_e = time.time() - t_i\n",
        "                    err[i,j], std[i,j] = model.test(X_data1['test'], Y_data['test'], GSO)\n",
        "                    params = arch.n_params if hasattr(arch, 'n_params') else None\n",
        "\n",
        "\n",
        "                elif exp['arc_p']['arch'] == DAGNN:\n",
        "                    X_data1, Y_data1 = DAGNN_model(Adj, X_data, Y_data)\n",
        "                    GSO = utils.select_GSO(arc_p, GSOs, sel_GSOs, W, Adj)\n",
        "                    K = GSO.shape[0] if isinstance(GSO, torch.Tensor) and len(GSO.shape) == 3 else 0\n",
        "                    arch = utils.instantiate_arch(arc_p, K)\n",
        "                    model = Model(arch, device=device)\n",
        "                    t_i = time.time()\n",
        "                    model.fit(X_data1, Y_data1, GSO, mod_p['lr'], mod_p['epochs'], mod_p['bs'], mod_p['wd'], patience=mod_p['pat'])\n",
        "                    t_e = time.time() - t_i\n",
        "                    err[i,j], std[i,j] = model.test(X_data1['test'], Y_data1['test'], GSO)\n",
        "                    params = arch.n_params if hasattr(arch, 'n_params') else None\n",
        "                else:\n",
        "                    # Fit and test nonlinear models\n",
        "                    GSO = utils.select_GSO(arc_p, GSOs, sel_GSOs, W, Adj)\n",
        "                    K = GSO.shape[0] if isinstance(GSO, torch.Tensor) and len(GSO.shape) == 3 else 0\n",
        "                    arch = utils.instantiate_arch(arc_p, K)\n",
        "                    model = Model(arch, device=device)\n",
        "                    t_i = time.time()\n",
        "                    model.fit(X_data, Y_data, GSO, mod_p['lr'], mod_p['epochs'], mod_p['bs'], mod_p['wd'], patience=mod_p['pat'])\n",
        "                    t_e = time.time() - t_i\n",
        "                    err[i,j], std[i,j] = model.test(X_data['test'], Y_data['test'], GSO)\n",
        "                    params = arch.n_params if hasattr(arch, 'n_params') else None\n",
        "\n",
        "                times[i,j] = t_e\n",
        "\n",
        "                # params = arch.n_params if hasattr(arch, 'n_params') else None\n",
        "\n",
        "                # Progress\n",
        "                pbar.update(1)\n",
        "                if verb:\n",
        "                    print(f'-{i}. {exp[\"leg\"]}: err: {err[i,j]:.3f} | std: {std[i,j]:.3f}  |' +\n",
        "                          f' time: {times[i,j]:.1f} | n_params: {params}')\n",
        "\n",
        "    total_t = (time.time() - t_begin)/60\n",
        "    print(f'----- Ellapsed time: {total_t:.2f} minutes -----')\n",
        "    return err, std, times\n",
        "\n",
        "P_MAX = \"max\"\n",
        "\n",
        "Exps = [\n",
        "    # Our Models\n",
        "\n",
        "    # {'arc_p': {'arch': ParallelMLPSum , 'GSO': 'GSOs', 'n_inputs': 20, 'input_dim': 20, 'hidden_dims': [32], 'output_dim': 20}, 'leg': 'ParallelMLPSum - 1 layer, hid_dim:32'},\n",
        "    # {'arc_p': {'arch': SharedMLPSum , 'GSO': 'GSOs', 'n_inputs': 20, 'input_dim': 20, 'hidden_dims': [32], 'output_dim': 20}, 'leg': 'SharedMLPSum - 1 layer, hid_dim:32'},\n",
        "    # {'arc_p': {'arch': SMLP2 , 'GSO': 'GSOs', 'in_dim': 1, 'hid_dim': [128], 'out_dim': 1, 'bias' : True }, 'leg': 'SMLP2-128'},\n",
        "    # {'arc_p': {'arch': SMLP , 'GSO': 'GSOs', 'in_dim': 1, 'hid_dim': [128], 'out_dim': 1, 'bias' : True }, 'leg': 'SMLP-128'},\n",
        "\n",
        "    # {'arc_p': {'arch': SMLP , 'GSO': 'GSOs', 'in_dim': 1, 'hid_dim': [256], 'out_dim': 1, 'bias' : True }, 'leg': 'SMLP-256'},\n",
        "    # {'arc_p': {'arch': SMLP , 'GSO': 'GSOs', 'in_dim': 1, 'hid_dim': [64], 'out_dim': 1, 'bias' : True }, 'leg': 'SMLP-64'},\n",
        "\n",
        "\n",
        "    # {'arc_p': {'arch': ParallelMLPSum , 'GSO': 'GSOs', 'n_inputs': 20, 'input_dim': 20, 'hidden_dims': [64], 'output_dim': 20}, 'leg': 'ParallelMLPSum - 1 layer, hid_dim:64'},\n",
        "    # {'arc_p': {'arch': SharedMLPSum , 'GSO': 'GSOs', 'n_inputs': 20, 'input_dim': 20, 'hidden_dims': [64], 'output_dim': 20}, 'leg': 'SharedMLPSum - 1 layer, hid_dim:64'},\n",
        "\n",
        "\n",
        "    {'arc_p': {'arch': FB_DAGConv, 'GSO': 'GSOs'}, 'leg': 'DCN'},\n",
        "\n",
        "    {'arc_p': {'arch': FB_DAGConv, 'GSO': 'rnd_GSOs', 'n_gsos': 15}, 'leg': 'DCN-15'},\n",
        "    {'arc_p': {'arch': SMLP , 'GSO': 'GSOs', 'in_dim': 1, 'hid_dim': [128], 'out_dim': 1, 'bias' : True }, 'leg': 'SMLP-128'},\n",
        "\n",
        "    # {'arc_p': {'arch': FB_DAGConv, 'GSO': 'rnd_GSOs', 'n_gsos': 10}, 'leg': 'DCN-10'},\n",
        "    # {'arc_p': {'arch': FB_DAGConv, 'GSO': 'rnd_GSOs', 'n_gsos': 5}, 'leg': 'DCN-5'},\n",
        "\n",
        "    # {'arc_p': {'arch': DAGConv, 'GSO': 'GSOs'}, 'leg': 'DAGConv'},\n",
        "    # {'arc_p': {'arch': DAGConv, 'GSO': 'rnd_GSOs', 'n_gsos': 15}, 'leg': 'DAGConv-15'},\n",
        "    # {'arc_p': {'arch': DAGConv, 'GSO': 'rnd_GSOs', 'n_gsos': 10}, 'leg': 'DAGConv-10'},\n",
        "    # {'arc_p': {'arch': DAGConv, 'GSO': 'rnd_GSOs', 'n_gsos': 5}, 'leg': 'DAGConv-5'},\n",
        "\n",
        "\n",
        "    {'arc_p': {'arch': FB_DAGConv, 'GSO': 'GSOs', 'transp': True}, 'leg': 'DCN-T'},\n",
        "\n",
        "\n",
        "    # {'arc_p': {'arch': FB_DAGConv, 'GSO': 'rnd_GSOs', 'n_gsos': 15, 'transp': True}, 'leg': 'DCN-15-T'},\n",
        "    # {'arc_p': {'arch': FB_DAGConv, 'GSO': 'rnd_GSOs', 'n_gsos': 10, 'transp': True}, 'leg': 'DCN-10-T'},\n",
        "\n",
        "    # {'arc_p': {'arch': DAGConv, 'GSO': 'GSOs', 'transp': True}, 'leg': 'DAGConv-T'},\n",
        "    # {'arc_p': {'arch': DAGConv, 'GSO': 'rnd_GSOs', 'n_gsos': 15, 'transp': True}, 'leg': 'DAGConv-15-T'},\n",
        "    # {'arc_p': {'arch': ADCN, 'GSO': 'GSOs', 'args': {'mlp_layers': 4}}, 'leg': 'ADCN-4'},\n",
        "\n",
        "    # {'arc_p': {'arch': ADCN, 'GSO': 'GSOs', 'transp': True, 'args': {'mlp_layers': 4}}, 'leg': 'ADCN-4-T'},\n",
        "    # {'arc_p': {'arch': ADCN, 'GSO': 'GSOs', 'transp': True, 'args': {'mlp_layers': 5}}, 'leg': 'ADCN-5-T'},\n",
        "\n",
        "\n",
        "    {'arc_p': {'arch': LinDAGRegModel, 'GSO': 'GSOs'}, 'leg': 'Linear'},\n",
        "    {'arc_p': {'arch': LinDAGRegModel, 'GSO': 'rnd_GSOs', 'n_gsos': 15}, 'leg': 'Linear-15'},\n",
        "\n",
        "\n",
        "\n",
        "    # {'arc_p': {'arch': LinDAGRegModel, 'GSO': 'rnd_GSOs', 'n_gsos': 10}, 'leg': 'Linear-10'},\n",
        "    # {'arc_p': {'arch': LinDAGRegModel, 'GSO': 'GSOs', 'transp': True}, 'leg': 'Linear-T'},\n",
        "    # {'arc_p': {'arch': LinDAGRegModel, 'GSO': 'rnd_GSOs', 'n_gsos': 15, 'transp': True}, 'leg': 'Linear-15-T'},\n",
        "    # {'arc_p': {'arch': LinDAGRegModel, 'GSO': 'rnd_GSOs', 'n_gsos': 10, 'transp': True}, 'leg': 'Linear-10-T'},\n",
        "\n",
        "\n",
        "    # {'arc_p': {'arch': ADCN, 'GSO': 'GSOs', 'args': {'mlp_layers': 4}}, 'leg': 'ADCN-4-2'},\n",
        "    # {'arc_p': {'arch': ADCN, 'GSO': 'GSOs', 'args': {'mlp_layers': 5}}, 'leg': 'ADCN-5-2'},\n",
        "    # {'arc_p': {'arch': ADCN, 'GSO': 'GSOs', 'args': {'mlp_layers': 4, 'n_layers': 4}}, 'leg': 'ADCN-4-4'},\n",
        "    # {'arc_p': {'arch': ADCN, 'GSO': 'GSOs', 'args': {'mlp_layers': 5, 'n_layers': 4}}, 'leg': 'ADCN-5-4'},\n",
        "\n",
        "\n",
        "\n",
        "    {'arc_p': {'arch': DAGNN, 'GSO': 'GSOs', 'emb_dim': 1, 'hidden_dim':128, 'out_dim': 128, 'max_n': 20,'nvt':1 ,\n",
        "               'START_TYPE': 0, 'END_TYPE': 1, 'hs':128, 'nz': 56, 'agg': \"attn_h\", 'num_layers':2, 'bidirectional': True, 'out_wx': False, 'out_pool_all': False,\n",
        "               'out_pool': P_MAX, 'dropout': 0.2, 'num_nodes': 20}, 'leg': 'DAGNN'},\n",
        "\n",
        "    {'arc_p': {'arch': DVAE, 'GSO': 'GSOs','max_n':20, 'nvt':1, 'START_TYPE':0, 'END_TYPE':1, 'hs':128,'nz':20, 'bidirectional': True, 'vid': True}, 'leg': 'DVAE'},\n",
        "\n",
        "\n",
        "    {'arc_p': {'arch': GraphSAGE, 'GSO': 'A-dgl', 'args': {'aggregator': 'mean'}}, 'leg': 'GraphSAGE-A'},\n",
        "    {'arc_p': {'arch': GIN, 'GSO': 'A-dgl', 'args': {'aggregator': 'sum'}}, 'leg': 'GIN-A'},\n",
        "    {'arc_p': {'arch': GIN, 'GSO': 'A-dgl', 'args': {'aggregator': 'sum', 'mlp_layers': 4}}, 'leg': 'GIN-A-4'},\n",
        "    {'arc_p': {'arch': MLP, 'GSO': None}, 'leg': 'MLP'},\n",
        "\n",
        "\n",
        "    # {'arc_p': {'arch': MLP, 'GSO': None, 'args': {'n_layers': 4}}, 'leg': 'MLP-4'},\n",
        "\n",
        "\n",
        "\n",
        "    {'arc_p': {'arch': MyGCNN, 'GSO': 'A'}, 'leg': 'GNN-A'},\n",
        "    {'arc_p': {'arch': FB_DAGConv, 'GSO': 'A_pows', 'K': 2, 'transp': False}, 'leg': 'FB-GCNN-2'},\n",
        "    # {'arc_p': {'arch': FB_DAGConv, 'GSO': 'A_pows', 'K': 3, 'transp': False}, 'leg': 'FB-GCNN-3'},\n",
        "    # {'arc_p': {'arch': FB_DAGConv, 'GSO': 'A_pows', 'K': 4, 'transp': False}, 'leg': 'FB-GCNN-4'},\n",
        "    {'arc_p': {'arch': GAT, 'GSO': 'A-dgl', 'args': {'num_heads': 2, 'hid_dim': 16, 'gat_params': {'attn_drop': 0}}},\n",
        "     'leg': 'GAT'},\n",
        "\n",
        "    ]\n",
        "\n",
        "mod_p_init = default_mod_p.copy()\n",
        "mod_p_init['pat'] = 50\n",
        "verb = True\n",
        "err_sulphate, std_sulphate, times_sulphate = run_exp(data_p, default_arch_args, mod_p_init, Exps, verb=verb)"
      ],
      "metadata": {
        "id": "j9Of2ZSnWKVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "# import dgl\n",
        "import torch\n",
        "from torch import nn\n",
        "import networkx as nx\n",
        "from tqdm.auto import tqdm\n",
        "import sys\n",
        "import os\n",
        "import pickle\n",
        "from numpy import linalg as la\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "\n",
        "\n",
        "import src.dag_utils as dagu\n",
        "import src.utils as utils\n",
        "from src.arch import DAGConv, FB_DAGConv, SF_DAGConv, ADCN , ParallelMLPSum, SharedMLPSum, SMLP\n",
        "from src.models import Model, LinDAGRegModel\n",
        "from src.baselines_archs import GAT, MLP, MyGCNN, GraphSAGE, GIN\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"dgl\")\n",
        "import os\n",
        "import igraph as ig\n",
        "from src.utils_Dagnn import *\n",
        "from src.DAGNN import DAGNN, DVAE\n",
        "\n",
        "\n",
        "\n",
        "# Ser random seed\n",
        "SEED = 10\n",
        "PATH = 'results/diffusion/'\n",
        "SAVE = True\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "# dgl.random.seed(SEED)\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "# torch.cuda.set_per_process_memory_fraction(.5, device=device)\n",
        "\n",
        "\n",
        "\n",
        "M = 343\n",
        "\n",
        "data_p = {\n",
        "    'n_tries': 25,  #25,\n",
        "\n",
        "    ## Graph parameters\n",
        "    'p': 0.2,  # .2                  # Edge prob in Erdos-Renyi DAG\n",
        "    'N': 20,                    # Number of nodes\n",
        "\n",
        "    ## Signal parameters\n",
        "    'M': M,                   # Number of observed signals\n",
        "    'M_train': int(0.7 * M),  # Samples selected for training\n",
        "    'M_val': int(0.2 * M),    # Samples selected for validation\n",
        "    'M_test': int(0.1 * M),   # Samples selected for test\n",
        "    'src_t': 'constant',          # 'random' or 'constant'\n",
        "    'max_src_node': 15, #25,           # Maximum index of nodes allowed to be sources\n",
        "    'n_sources': 14,             # Maximum Number of source nodes\n",
        "    'n_p_x': .05,\n",
        "    'n_p_y': .05,                 # Normalized noise power\n",
        "    'max_GSO': 20,              # Maximum index of GSOs involved in the diffusion\n",
        "    'min_GSO': 0,               # Minimum index of GSOs involved in the diffusion\n",
        "    'n_GSOs': 20,            # Number of GSOs\n",
        "    'concentration': 'nitrate.npy'\n",
        "}\n",
        "\n",
        "default_mod_p = {\n",
        "    'bs': 25,           # Size of the batch\n",
        "    'lr': 5e-4,         # Learning rate\n",
        "    'epochs': 100,  #50,       # Number of training epochs\n",
        "    'pat': 25,  # 15        # Number of non-decreasing epoch to stop training\n",
        "    'wd': 1e-4,         # Weight decay\n",
        "}\n",
        "\n",
        "default_arch_args = {\n",
        "    'in_dim': 1,        # Input dimension\n",
        "    'hid_dim': 32,     # Hidden dimension\n",
        "    'out_dim': 1,       # Output dimension\n",
        "    'n_layers': 2,#2,  # 3 also works well          # Number of layers\n",
        "    'l_act': None,\n",
        "    'bias': True,\n",
        "}\n",
        "\n",
        "\n",
        "def add_noise(signal, n_p):\n",
        "    shape = signal.shape\n",
        "\n",
        "    M = shape[0]\n",
        "    N = shape[1]\n",
        "\n",
        "    if n_p <= 0:\n",
        "        return signal\n",
        "\n",
        "    signal_norm = torch.norm(signal, p=2, dim=1, keepdim=True)\n",
        "    signal_norm[signal_norm == 0] = 1\n",
        "    noise = torch.randn(M, N, 1, device=signal.device)\n",
        "    noise_norm = torch.norm(noise, p=2, dim=1, keepdim=True)\n",
        "    noise = noise * signal_norm * torch.sqrt(torch.tensor(n_p)) / noise_norm\n",
        "\n",
        "    return signal + noise\n",
        "\n",
        "\n",
        "\n",
        "def get_real_data(d_dat_p ,get_Psi=False):\n",
        "\n",
        "    nodes = ['CH','CL','CN','CU','EN','EV','LE','LO','OC','PA','RA','TH','TM','WI','KE','TN','TS','TW','TSO','TR']\n",
        "\n",
        "    Adj = np.load('/content/drive/MyDrive/Thames/'+'adj_matrix.npy')\n",
        "\n",
        "\n",
        "    W = la.inv(np.eye(d_dat_p['N']) - Adj)\n",
        "    W_inf = la.inv(W)\n",
        "    dag = nx.from_numpy_array(Adj.T, create_using=nx.DiGraph())\n",
        "    if get_Psi:\n",
        "        Psi = np.array([dagu.compute_Dq(dag, i, d_dat_p['N']) for i in range(d_dat_p['N'])]).T\n",
        "        GSOs = np.array([(W * Psi[:,i]) @ W_inf for i in range(d_dat_p['N'])])\n",
        "        return Adj, W, GSOs, Psi\n",
        "    GSOs = np.array([(W * dagu.compute_Dq(dag, i, d_dat_p['N'])) @ W_inf for i in range(d_dat_p['N'])])\n",
        "\n",
        "    return Adj, W, GSOs\n",
        "\n",
        "\n",
        "def is_upper_triangular(adj_matrix):\n",
        "    return np.allclose(adj_matrix, np.triu(adj_matrix))\n",
        "\n",
        "def randomize_dag_adj(A_true, alpha):\n",
        "    num_nodes = A_true.shape[0]\n",
        "\n",
        "    while True:\n",
        "        A_rand = np.triu(A_true.copy())\n",
        "\n",
        "        total_possible_edges = (num_nodes * (num_nodes - 1)) // 2\n",
        "        num_modifications = int(alpha * total_possible_edges)\n",
        "\n",
        "        possible_edges = [(i, j) for i in range(num_nodes) for j in range(i+1, num_nodes)]\n",
        "\n",
        "        modifications = 0\n",
        "        while modifications < num_modifications:\n",
        "            # Randomly choose to add or remove an edge\n",
        "            action = random.choice(['add', 'remove'])\n",
        "            i, j = random.choice(possible_edges)\n",
        "\n",
        "            if action == 'add' and A_rand[i, j] == 0:\n",
        "                A_rand[i, j] = 1\n",
        "                modifications += 1\n",
        "            elif action == 'remove' and A_rand[i, j] == 1:\n",
        "                A_rand[i, j] = 0\n",
        "                modifications += 1\n",
        "\n",
        "        if is_upper_triangular(A_rand):\n",
        "            return A_rand\n",
        "\n",
        "\n",
        "def Thames(d_p,GSOs, pollutant=None):\n",
        "\n",
        "    range_GSO = np.arange(d_p['min_GSO'], d_p['max_GSO'])\n",
        "    gsos_idx = np.random.choice(range_GSO, size=d_p['n_GSOs'], replace=False)\n",
        "    sel_GSOs = GSOs[gsos_idx]\n",
        "    t1 = np.load('/content/drive/MyDrive/Thames/' + pollutant)\n",
        "    Yn_t = torch.tensor(t1, dtype = torch.float32)\n",
        "\n",
        "    Xn_t = Yn_t.clone()\n",
        "    Xn_t[:,-6:,:]  = 0\n",
        "\n",
        "    Xn_t = add_noise(Xn_t, 0.00)\n",
        "    Yn_t = add_noise(Yn_t, 0.00)\n",
        "\n",
        "    X_data = {'train': Xn_t[:d_p['M_train']], 'val': Xn_t[d_p['M_train']:-d_p['M_test']], 'test': Xn_t[-d_p['M_test']:]}\n",
        "    Y_data = {'train': Yn_t[:d_p['M_train']], 'val': Yn_t[d_p['M_train']:-d_p['M_test']], 'test': Yn_t[-d_p['M_test']:]}\n",
        "\n",
        "    return X_data, Y_data, sel_GSOs, gsos_idx\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_signals(d_p, GSOs):\n",
        "    range_GSO = np.arange(d_p['min_GSO'], d_p['max_GSO'])\n",
        "    gsos_idx = np.random.choice(range_GSO, size=d_p['n_GSOs'], replace=False)\n",
        "    sel_GSOs = GSOs[gsos_idx]\n",
        "    Yn_t, X_t, Y_t = dagu.create_diff_data(d_p['M'], sel_GSOs, d_p['max_src_node'], d_p['n_p_x'], d_p['n_p_y'],\n",
        "                                           d_p['n_sources'], src_t=d_p['src_t'], torch_tensor=True, verb=False)\n",
        "\n",
        "\n",
        "\n",
        "    X_data = {'train': X_t[:d_p['M_train']], 'val': X_t[d_p['M_train']:-d_p['M_test']], 'test': X_t[-d_p['M_test']:]}\n",
        "    Y_data = {'train': Yn_t[:d_p['M_train']], 'val': Yn_t[d_p['M_train']:-d_p['M_test']],\n",
        "              'test': Y_t[-d_p['M_test']:]}\n",
        "\n",
        "    return X_data, Y_data, sel_GSOs, gsos_idx\n",
        "\n",
        "\n",
        "def run_exp(d_p, d_arc_args, d_mod_p, exps, verb=True):\n",
        "    # Create error variables\n",
        "    print(d_p['concentration'])\n",
        "    err = np.zeros((d_p['n_tries'], len(exps)))\n",
        "    std = np.zeros((d_p['n_tries'], len(exps)))\n",
        "    times = np.zeros((d_p['n_tries'], len(exps)))\n",
        "\n",
        "    t_begin = time.time()\n",
        "    # for i in range(d_p['n_tries']):\n",
        "    with tqdm(total=d_p['n_tries']*len(exps), disable=False) as pbar:\n",
        "        for i in range(d_p['n_tries']):\n",
        "            Adj, W, GSOs, Psi = get_real_data(d_p, get_Psi=True)\n",
        "\n",
        "            X_data, Y_data, sel_GSOs, sel_GSOs_idx = Thames(d_p, GSOs, d_p['concentration'])\n",
        "\n",
        "            for j, exp in enumerate(exps):\n",
        "                arc_p = {**exp['arc_p']}\n",
        "\n",
        "                arc_p['args'] = {**d_arc_args, **arc_p['args']} if 'args' in arc_p.keys() else {**d_arc_args}\n",
        "                mod_p = {**d_mod_p, **exp['mod_p']} if 'mod_p' in exp.keys() else d_mod_p\n",
        "\n",
        "                if exp['arc_p']['arch'] == LinDAGRegModel:\n",
        "                    # Fit and test linear model\n",
        "                    if 'transp' in arc_p.keys() and arc_p['transp']:\n",
        "                        dag_T = nx.from_numpy_array(Adj, create_using=nx.DiGraph())\n",
        "                        Psi = np.array([dagu.compute_Dq(dag_T, i, d_p['N']) for i in range(d_p['N'])]).T\n",
        "                        arc_p['transp'] = False\n",
        "\n",
        "                    Psi_sel = utils.select_GSO(arc_p, Psi.T, Psi[:,sel_GSOs_idx].T, W, Adj, sel_GSOs_idx).numpy().T\n",
        "                    lin_model = LinDAGRegModel(W, Psi_sel)\n",
        "\n",
        "                    t_i = time.time()\n",
        "                    lin_model.fit(X_data['train'], Y_data['train'])\n",
        "                    t_e = time.time() - t_i\n",
        "                    err[i,j], std[i,j] = lin_model.test(X_data['test'], Y_data['test'])\n",
        "                    params = lin_model.n_params if hasattr(lin_model, 'n_params') else None\n",
        "\n",
        "\n",
        "                elif exp['arc_p']['arch'] == DVAE:\n",
        "\n",
        "                    X_data1 = DVAE_exp(Adj, X_data)\n",
        "                    GSO = utils.select_GSO(arc_p, GSOs, sel_GSOs, W, Adj)\n",
        "                    K = GSO.shape[0] if isinstance(GSO, torch.Tensor) and len(GSO.shape) == 3 else 0\n",
        "                    arch = utils.instantiate_arch(arc_p, K)\n",
        "                    model = Model(arch, device=device)\n",
        "\n",
        "                    t_i = time.time()\n",
        "                    model.fit(X_data1, Y_data, GSO, mod_p['lr'], mod_p['epochs'], mod_p['bs'], mod_p['wd'], patience=mod_p['pat'])\n",
        "                    t_e = time.time() - t_i\n",
        "                    err[i,j], std[i,j] = model.test(X_data1['test'], Y_data['test'], GSO)\n",
        "                    params = arch.n_params if hasattr(arch, 'n_params') else None\n",
        "\n",
        "\n",
        "                elif exp['arc_p']['arch'] == DAGNN:\n",
        "                    X_data1, Y_data1 = DAGNN_model(Adj, X_data, Y_data)\n",
        "                    GSO = utils.select_GSO(arc_p, GSOs, sel_GSOs, W, Adj)\n",
        "                    K = GSO.shape[0] if isinstance(GSO, torch.Tensor) and len(GSO.shape) == 3 else 0\n",
        "                    arch = utils.instantiate_arch(arc_p, K)\n",
        "                    model = Model(arch, device=device)\n",
        "                    t_i = time.time()\n",
        "                    model.fit(X_data1, Y_data1, GSO, mod_p['lr'], mod_p['epochs'], mod_p['bs'], mod_p['wd'], patience=mod_p['pat'])\n",
        "                    t_e = time.time() - t_i\n",
        "                    err[i,j], std[i,j] = model.test(X_data1['test'], Y_data1['test'], GSO)\n",
        "                    params = arch.n_params if hasattr(arch, 'n_params') else None\n",
        "                else:\n",
        "                    # Fit and test nonlinear models\n",
        "                    GSO = utils.select_GSO(arc_p, GSOs, sel_GSOs, W, Adj)\n",
        "                    K = GSO.shape[0] if isinstance(GSO, torch.Tensor) and len(GSO.shape) == 3 else 0\n",
        "                    arch = utils.instantiate_arch(arc_p, K)\n",
        "                    model = Model(arch, device=device)\n",
        "                    t_i = time.time()\n",
        "                    model.fit(X_data, Y_data, GSO, mod_p['lr'], mod_p['epochs'], mod_p['bs'], mod_p['wd'], patience=mod_p['pat'])\n",
        "                    t_e = time.time() - t_i\n",
        "                    err[i,j], std[i,j] = model.test(X_data['test'], Y_data['test'], GSO)\n",
        "                    params = arch.n_params if hasattr(arch, 'n_params') else None\n",
        "\n",
        "                times[i,j] = t_e\n",
        "\n",
        "                # params = arch.n_params if hasattr(arch, 'n_params') else None\n",
        "\n",
        "                # Progress\n",
        "                pbar.update(1)\n",
        "                if verb:\n",
        "                    print(f'-{i}. {exp[\"leg\"]}: err: {err[i,j]:.3f} | std: {std[i,j]:.3f}  |' +\n",
        "                          f' time: {times[i,j]:.1f} | n_params: {params}')\n",
        "\n",
        "    total_t = (time.time() - t_begin)/60\n",
        "    print(f'----- Ellapsed time: {total_t:.2f} minutes -----')\n",
        "    return err, std, times\n",
        "\n",
        "\n",
        "\n",
        "Exps = [\n",
        "    # Our Models\n",
        "\n",
        "    # {'arc_p': {'arch': ParallelMLPSum , 'GSO': 'GSOs', 'n_inputs': 20, 'input_dim': 20, 'hidden_dims': [32], 'output_dim': 20}, 'leg': 'ParallelMLPSum - 1 layer, hid_dim:32'},\n",
        "    # {'arc_p': {'arch': SharedMLPSum , 'GSO': 'GSOs', 'n_inputs': 20, 'input_dim': 20, 'hidden_dims': [32], 'output_dim': 20}, 'leg': 'SharedMLPSum - 1 layer, hid_dim:32'},\n",
        "    # {'arc_p': {'arch': SMLP2 , 'GSO': 'GSOs', 'in_dim': 1, 'hid_dim': [128], 'out_dim': 1, 'bias' : True }, 'leg': 'SMLP2-128'},\n",
        "    # {'arc_p': {'arch': SMLP , 'GSO': 'GSOs', 'in_dim': 1, 'hid_dim': [128], 'out_dim': 1, 'bias' : True }, 'leg': 'SMLP-128'},\n",
        "\n",
        "    # {'arc_p': {'arch': SMLP , 'GSO': 'GSOs', 'in_dim': 1, 'hid_dim': [256], 'out_dim': 1, 'bias' : True }, 'leg': 'SMLP-256'},\n",
        "    # {'arc_p': {'arch': SMLP , 'GSO': 'GSOs', 'in_dim': 1, 'hid_dim': [64], 'out_dim': 1, 'bias' : True }, 'leg': 'SMLP-64'},\n",
        "\n",
        "\n",
        "    # {'arc_p': {'arch': ParallelMLPSum , 'GSO': 'GSOs', 'n_inputs': 20, 'input_dim': 20, 'hidden_dims': [64], 'output_dim': 20}, 'leg': 'ParallelMLPSum - 1 layer, hid_dim:64'},\n",
        "    # {'arc_p': {'arch': SharedMLPSum , 'GSO': 'GSOs', 'n_inputs': 20, 'input_dim': 20, 'hidden_dims': [64], 'output_dim': 20}, 'leg': 'SharedMLPSum - 1 layer, hid_dim:64'},\n",
        "\n",
        "\n",
        "    {'arc_p': {'arch': FB_DAGConv, 'GSO': 'GSOs'}, 'leg': 'DCN'},\n",
        "\n",
        "    {'arc_p': {'arch': FB_DAGConv, 'GSO': 'rnd_GSOs', 'n_gsos': 15}, 'leg': 'DCN-15'},\n",
        "    {'arc_p': {'arch': SMLP , 'GSO': 'GSOs', 'in_dim': 1, 'hid_dim': [128], 'out_dim': 1, 'bias' : True }, 'leg': 'SMLP-128'},\n",
        "\n",
        "    # {'arc_p': {'arch': FB_DAGConv, 'GSO': 'rnd_GSOs', 'n_gsos': 10}, 'leg': 'DCN-10'},\n",
        "    # {'arc_p': {'arch': FB_DAGConv, 'GSO': 'rnd_GSOs', 'n_gsos': 5}, 'leg': 'DCN-5'},\n",
        "\n",
        "    # {'arc_p': {'arch': DAGConv, 'GSO': 'GSOs'}, 'leg': 'DAGConv'},\n",
        "    # {'arc_p': {'arch': DAGConv, 'GSO': 'rnd_GSOs', 'n_gsos': 15}, 'leg': 'DAGConv-15'},\n",
        "    # {'arc_p': {'arch': DAGConv, 'GSO': 'rnd_GSOs', 'n_gsos': 10}, 'leg': 'DAGConv-10'},\n",
        "    # {'arc_p': {'arch': DAGConv, 'GSO': 'rnd_GSOs', 'n_gsos': 5}, 'leg': 'DAGConv-5'},\n",
        "\n",
        "\n",
        "    {'arc_p': {'arch': FB_DAGConv, 'GSO': 'GSOs', 'transp': True}, 'leg': 'DCN-T'},\n",
        "\n",
        "\n",
        "    # {'arc_p': {'arch': FB_DAGConv, 'GSO': 'rnd_GSOs', 'n_gsos': 15, 'transp': True}, 'leg': 'DCN-15-T'},\n",
        "    # {'arc_p': {'arch': FB_DAGConv, 'GSO': 'rnd_GSOs', 'n_gsos': 10, 'transp': True}, 'leg': 'DCN-10-T'},\n",
        "\n",
        "    # {'arc_p': {'arch': DAGConv, 'GSO': 'GSOs', 'transp': True}, 'leg': 'DAGConv-T'},\n",
        "    # {'arc_p': {'arch': DAGConv, 'GSO': 'rnd_GSOs', 'n_gsos': 15, 'transp': True}, 'leg': 'DAGConv-15-T'},\n",
        "    # {'arc_p': {'arch': ADCN, 'GSO': 'GSOs', 'args': {'mlp_layers': 4}}, 'leg': 'ADCN-4'},\n",
        "\n",
        "    # {'arc_p': {'arch': ADCN, 'GSO': 'GSOs', 'transp': True, 'args': {'mlp_layers': 4}}, 'leg': 'ADCN-4-T'},\n",
        "    # {'arc_p': {'arch': ADCN, 'GSO': 'GSOs', 'transp': True, 'args': {'mlp_layers': 5}}, 'leg': 'ADCN-5-T'},\n",
        "\n",
        "\n",
        "    {'arc_p': {'arch': LinDAGRegModel, 'GSO': 'GSOs'}, 'leg': 'Linear'},\n",
        "    {'arc_p': {'arch': LinDAGRegModel, 'GSO': 'rnd_GSOs', 'n_gsos': 15}, 'leg': 'Linear-15'},\n",
        "\n",
        "\n",
        "\n",
        "    # {'arc_p': {'arch': LinDAGRegModel, 'GSO': 'rnd_GSOs', 'n_gsos': 10}, 'leg': 'Linear-10'},\n",
        "    # {'arc_p': {'arch': LinDAGRegModel, 'GSO': 'GSOs', 'transp': True}, 'leg': 'Linear-T'},\n",
        "    # {'arc_p': {'arch': LinDAGRegModel, 'GSO': 'rnd_GSOs', 'n_gsos': 15, 'transp': True}, 'leg': 'Linear-15-T'},\n",
        "    # {'arc_p': {'arch': LinDAGRegModel, 'GSO': 'rnd_GSOs', 'n_gsos': 10, 'transp': True}, 'leg': 'Linear-10-T'},\n",
        "\n",
        "\n",
        "    # {'arc_p': {'arch': ADCN, 'GSO': 'GSOs', 'args': {'mlp_layers': 4}}, 'leg': 'ADCN-4-2'},\n",
        "    # {'arc_p': {'arch': ADCN, 'GSO': 'GSOs', 'args': {'mlp_layers': 5}}, 'leg': 'ADCN-5-2'},\n",
        "    # {'arc_p': {'arch': ADCN, 'GSO': 'GSOs', 'args': {'mlp_layers': 4, 'n_layers': 4}}, 'leg': 'ADCN-4-4'},\n",
        "    # {'arc_p': {'arch': ADCN, 'GSO': 'GSOs', 'args': {'mlp_layers': 5, 'n_layers': 4}}, 'leg': 'ADCN-5-4'},\n",
        "    {'arc_p': {'arch': DAGNN, 'GSO': 'GSOs', 'emb_dim': 1, 'hidden_dim':128, 'out_dim': 128, 'max_n': 20,'nvt':1 ,\n",
        "               'START_TYPE': 0, 'END_TYPE': 1, 'hs':128, 'nz': 56, 'agg': \"attn_h\", 'num_layers':2, 'bidirectional': True, 'out_wx': False, 'out_pool_all': False,\n",
        "               'out_pool': P_MAX, 'dropout': 0.2, 'num_nodes': 20}, 'leg': 'DAGNN'},\n",
        "\n",
        "    {'arc_p': {'arch': DVAE, 'GSO': 'GSOs','max_n':20, 'nvt':1, 'START_TYPE':0, 'END_TYPE':1, 'hs':128,'nz':20, 'bidirectional': True, 'vid': True}, 'leg': 'DVAE'},\n",
        "\n",
        "\n",
        "    {'arc_p': {'arch': GraphSAGE, 'GSO': 'A-dgl', 'args': {'aggregator': 'mean'}}, 'leg': 'GraphSAGE-A'},\n",
        "    {'arc_p': {'arch': GIN, 'GSO': 'A-dgl', 'args': {'aggregator': 'sum'}}, 'leg': 'GIN-A'},\n",
        "    {'arc_p': {'arch': GIN, 'GSO': 'A-dgl', 'args': {'aggregator': 'sum', 'mlp_layers': 4}}, 'leg': 'GIN-A-4'},\n",
        "    {'arc_p': {'arch': MLP, 'GSO': None}, 'leg': 'MLP'},\n",
        "\n",
        "\n",
        "    # {'arc_p': {'arch': MLP, 'GSO': None, 'args': {'n_layers': 4}}, 'leg': 'MLP-4'},\n",
        "\n",
        "\n",
        "\n",
        "    {'arc_p': {'arch': MyGCNN, 'GSO': 'A'}, 'leg': 'GNN-A'},\n",
        "    {'arc_p': {'arch': FB_DAGConv, 'GSO': 'A_pows', 'K': 2, 'transp': False}, 'leg': 'FB-GCNN-2'},\n",
        "    # {'arc_p': {'arch': FB_DAGConv, 'GSO': 'A_pows', 'K': 3, 'transp': False}, 'leg': 'FB-GCNN-3'},\n",
        "    # {'arc_p': {'arch': FB_DAGConv, 'GSO': 'A_pows', 'K': 4, 'transp': False}, 'leg': 'FB-GCNN-4'},\n",
        "    {'arc_p': {'arch': GAT, 'GSO': 'A-dgl', 'args': {'num_heads': 2, 'hid_dim': 16, 'gat_params': {'attn_drop': 0}}},\n",
        "     'leg': 'GAT'},\n",
        "\n",
        "    ]\n",
        "\n",
        "mod_p_init = default_mod_p.copy()\n",
        "mod_p_init['pat'] = 50\n",
        "verb = True\n",
        "err_nitrate, std_nitrate, times_nitrate = run_exp(data_p, default_arch_args, mod_p_init, Exps, verb=verb)"
      ],
      "metadata": {
        "id": "_gP6jk4MWM7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "# import dgl\n",
        "import torch\n",
        "from torch import nn\n",
        "import networkx as nx\n",
        "from tqdm.auto import tqdm\n",
        "import sys\n",
        "import os\n",
        "import pickle\n",
        "from numpy import linalg as la\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "\n",
        "\n",
        "import src.dag_utils as dagu\n",
        "import src.utils as utils\n",
        "from src.arch import DAGConv, FB_DAGConv, SF_DAGConv, ADCN , ParallelMLPSum, SharedMLPSum, SMLP\n",
        "from src.models import Model, LinDAGRegModel\n",
        "from src.baselines_archs import GAT, MLP, MyGCNN, GraphSAGE, GIN\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"dgl\")\n",
        "import os\n",
        "import igraph as ig\n",
        "from src.utils_Dagnn import *\n",
        "from src.DAGNN import DAGNN, DVAE\n",
        "\n",
        "\n",
        "\n",
        "# Ser random seed\n",
        "SEED = 10\n",
        "PATH = 'results/diffusion/'\n",
        "SAVE = True\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "# dgl.random.seed(SEED)\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "# torch.cuda.set_per_process_memory_fraction(.5, device=device)\n",
        "\n",
        "\n",
        "\n",
        "M = 343\n",
        "\n",
        "data_p = {\n",
        "    'n_tries': 25,  #25,\n",
        "\n",
        "    ## Graph parameters\n",
        "    'p': 0.2,  # .2                  # Edge prob in Erdos-Renyi DAG\n",
        "    'N': 20,                    # Number of nodes\n",
        "\n",
        "    ## Signal parameters\n",
        "    'M': M,                   # Number of observed signals\n",
        "    'M_train': int(0.7 * M),  # Samples selected for training\n",
        "    'M_val': int(0.2 * M),    # Samples selected for validation\n",
        "    'M_test': int(0.1 * M),   # Samples selected for test\n",
        "    'src_t': 'constant',          # 'random' or 'constant'\n",
        "    'max_src_node': 15, #25,           # Maximum index of nodes allowed to be sources\n",
        "    'n_sources': 14,             # Maximum Number of source nodes\n",
        "    'n_p_x': .05,\n",
        "    'n_p_y': .05,                 # Normalized noise power\n",
        "    'max_GSO': 20,              # Maximum index of GSOs involved in the diffusion\n",
        "    'min_GSO': 0,               # Minimum index of GSOs involved in the diffusion\n",
        "    'n_GSOs': 20,            # Number of GSOs\n",
        "    'concentration': 'chloride.npy'\n",
        "}\n",
        "\n",
        "default_mod_p = {\n",
        "    'bs': 25,           # Size of the batch\n",
        "    'lr': 5e-4,         # Learning rate\n",
        "    'epochs': 100,  #50,       # Number of training epochs\n",
        "    'pat': 25,  # 15        # Number of non-decreasing epoch to stop training\n",
        "    'wd': 1e-4,         # Weight decay\n",
        "}\n",
        "\n",
        "default_arch_args = {\n",
        "    'in_dim': 1,        # Input dimension\n",
        "    'hid_dim': 32,     # Hidden dimension\n",
        "    'out_dim': 1,       # Output dimension\n",
        "    'n_layers': 2,#2,  # 3 also works well          # Number of layers\n",
        "    'l_act': None,\n",
        "    'bias': True,\n",
        "}\n",
        "\n",
        "\n",
        "def add_noise(signal, n_p):\n",
        "    shape = signal.shape\n",
        "\n",
        "    M = shape[0]\n",
        "    N = shape[1]\n",
        "\n",
        "    if n_p <= 0:\n",
        "        return signal\n",
        "\n",
        "    signal_norm = torch.norm(signal, p=2, dim=1, keepdim=True)\n",
        "    signal_norm[signal_norm == 0] = 1\n",
        "    noise = torch.randn(M, N, 1, device=signal.device)\n",
        "    noise_norm = torch.norm(noise, p=2, dim=1, keepdim=True)\n",
        "    noise = noise * signal_norm * torch.sqrt(torch.tensor(n_p)) / noise_norm\n",
        "\n",
        "    return signal + noise\n",
        "\n",
        "\n",
        "\n",
        "def get_real_data(d_dat_p ,get_Psi=False):\n",
        "\n",
        "    nodes = ['CH','CL','CN','CU','EN','EV','LE','LO','OC','PA','RA','TH','TM','WI','KE','TN','TS','TW','TSO','TR']\n",
        "\n",
        "    Adj = np.load('/content/drive/MyDrive/Thames/'+'adj_matrix.npy')\n",
        "\n",
        "    W = la.inv(np.eye(d_dat_p['N']) - Adj)\n",
        "    W_inf = la.inv(W)\n",
        "    dag = nx.from_numpy_array(Adj.T, create_using=nx.DiGraph())\n",
        "    if get_Psi:\n",
        "        Psi = np.array([dagu.compute_Dq(dag, i, d_dat_p['N']) for i in range(d_dat_p['N'])]).T\n",
        "        GSOs = np.array([(W * Psi[:,i]) @ W_inf for i in range(d_dat_p['N'])])\n",
        "        return Adj, W, GSOs, Psi\n",
        "    GSOs = np.array([(W * dagu.compute_Dq(dag, i, d_dat_p['N'])) @ W_inf for i in range(d_dat_p['N'])])\n",
        "\n",
        "    return Adj, W, GSOs\n",
        "\n",
        "\n",
        "def is_upper_triangular(adj_matrix):\n",
        "    return np.allclose(adj_matrix, np.triu(adj_matrix))\n",
        "\n",
        "def randomize_dag_adj(A_true, alpha):\n",
        "    num_nodes = A_true.shape[0]\n",
        "\n",
        "    while True:\n",
        "        A_rand = np.triu(A_true.copy())\n",
        "\n",
        "        total_possible_edges = (num_nodes * (num_nodes - 1)) // 2\n",
        "        num_modifications = int(alpha * total_possible_edges)\n",
        "\n",
        "        possible_edges = [(i, j) for i in range(num_nodes) for j in range(i+1, num_nodes)]\n",
        "\n",
        "        modifications = 0\n",
        "        while modifications < num_modifications:\n",
        "            # Randomly choose to add or remove an edge\n",
        "            action = random.choice(['add', 'remove'])\n",
        "            i, j = random.choice(possible_edges)\n",
        "\n",
        "            if action == 'add' and A_rand[i, j] == 0:\n",
        "                A_rand[i, j] = 1\n",
        "                modifications += 1\n",
        "            elif action == 'remove' and A_rand[i, j] == 1:\n",
        "                A_rand[i, j] = 0\n",
        "                modifications += 1\n",
        "\n",
        "        if is_upper_triangular(A_rand):\n",
        "            return A_rand\n",
        "\n",
        "\n",
        "def Thames(d_p,GSOs, pollutant=None):\n",
        "\n",
        "    range_GSO = np.arange(d_p['min_GSO'], d_p['max_GSO'])\n",
        "    gsos_idx = np.random.choice(range_GSO, size=d_p['n_GSOs'], replace=False)\n",
        "    sel_GSOs = GSOs[gsos_idx]\n",
        "    t1 = np.load('/content/drive/MyDrive/Thames/' + pollutant)\n",
        "    Yn_t = torch.tensor(t1, dtype = torch.float32)\n",
        "\n",
        "    Xn_t = Yn_t.clone()\n",
        "    Xn_t[:,-6:,:]  = 0\n",
        "\n",
        "    Xn_t = add_noise(Xn_t, 0.00)\n",
        "    Yn_t = add_noise(Yn_t, 0.00)\n",
        "\n",
        "    X_data = {'train': Xn_t[:d_p['M_train']], 'val': Xn_t[d_p['M_train']:-d_p['M_test']], 'test': Xn_t[-d_p['M_test']:]}\n",
        "    Y_data = {'train': Yn_t[:d_p['M_train']], 'val': Yn_t[d_p['M_train']:-d_p['M_test']], 'test': Yn_t[-d_p['M_test']:]}\n",
        "\n",
        "    return X_data, Y_data, sel_GSOs, gsos_idx\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_signals(d_p, GSOs):\n",
        "    range_GSO = np.arange(d_p['min_GSO'], d_p['max_GSO'])\n",
        "    gsos_idx = np.random.choice(range_GSO, size=d_p['n_GSOs'], replace=False)\n",
        "    sel_GSOs = GSOs[gsos_idx]\n",
        "    Yn_t, X_t, Y_t = dagu.create_diff_data(d_p['M'], sel_GSOs, d_p['max_src_node'], d_p['n_p_x'], d_p['n_p_y'],\n",
        "                                           d_p['n_sources'], src_t=d_p['src_t'], torch_tensor=True, verb=False)\n",
        "\n",
        "\n",
        "\n",
        "    X_data = {'train': X_t[:d_p['M_train']], 'val': X_t[d_p['M_train']:-d_p['M_test']], 'test': X_t[-d_p['M_test']:]}\n",
        "    Y_data = {'train': Yn_t[:d_p['M_train']], 'val': Yn_t[d_p['M_train']:-d_p['M_test']],\n",
        "              'test': Y_t[-d_p['M_test']:]}\n",
        "\n",
        "    return X_data, Y_data, sel_GSOs, gsos_idx\n",
        "\n",
        "\n",
        "def run_exp(d_p, d_arc_args, d_mod_p, exps, verb=True):\n",
        "    # Create error variables\n",
        "    print(d_p['concentration'])\n",
        "    err = np.zeros((d_p['n_tries'], len(exps)))\n",
        "    std = np.zeros((d_p['n_tries'], len(exps)))\n",
        "    times = np.zeros((d_p['n_tries'], len(exps)))\n",
        "\n",
        "    t_begin = time.time()\n",
        "    # for i in range(d_p['n_tries']):\n",
        "    with tqdm(total=d_p['n_tries']*len(exps), disable=False) as pbar:\n",
        "        for i in range(d_p['n_tries']):\n",
        "            Adj, W, GSOs, Psi = get_real_data(d_p, get_Psi=True)\n",
        "\n",
        "            X_data, Y_data, sel_GSOs, sel_GSOs_idx = Thames(d_p, GSOs, d_p['concentration'])\n",
        "\n",
        "            for j, exp in enumerate(exps):\n",
        "                arc_p = {**exp['arc_p']}\n",
        "\n",
        "                arc_p['args'] = {**d_arc_args, **arc_p['args']} if 'args' in arc_p.keys() else {**d_arc_args}\n",
        "                mod_p = {**d_mod_p, **exp['mod_p']} if 'mod_p' in exp.keys() else d_mod_p\n",
        "\n",
        "                if exp['arc_p']['arch'] == LinDAGRegModel:\n",
        "                    # Fit and test linear model\n",
        "                    if 'transp' in arc_p.keys() and arc_p['transp']:\n",
        "                        dag_T = nx.from_numpy_array(Adj, create_using=nx.DiGraph())\n",
        "                        Psi = np.array([dagu.compute_Dq(dag_T, i, d_p['N']) for i in range(d_p['N'])]).T\n",
        "                        arc_p['transp'] = False\n",
        "\n",
        "                    Psi_sel = utils.select_GSO(arc_p, Psi.T, Psi[:,sel_GSOs_idx].T, W, Adj, sel_GSOs_idx).numpy().T\n",
        "                    lin_model = LinDAGRegModel(W, Psi_sel)\n",
        "\n",
        "                    t_i = time.time()\n",
        "                    lin_model.fit(X_data['train'], Y_data['train'])\n",
        "                    t_e = time.time() - t_i\n",
        "                    err[i,j], std[i,j] = lin_model.test(X_data['test'], Y_data['test'])\n",
        "                    params = lin_model.n_params if hasattr(lin_model, 'n_params') else None\n",
        "\n",
        "\n",
        "                elif exp['arc_p']['arch'] == DVAE:\n",
        "                    X_data1 = DVAE_exp(Adj, X_data)\n",
        "                    GSO = utils.select_GSO(arc_p, GSOs, sel_GSOs, W, Adj)\n",
        "                    K = GSO.shape[0] if isinstance(GSO, torch.Tensor) and len(GSO.shape) == 3 else 0\n",
        "                    arch = utils.instantiate_arch(arc_p, K)\n",
        "                    model = Model(arch, device=device)\n",
        "\n",
        "                    t_i = time.time()\n",
        "                    model.fit(X_data1, Y_data, GSO, mod_p['lr'], mod_p['epochs'], mod_p['bs'], mod_p['wd'], patience=mod_p['pat'])\n",
        "                    t_e = time.time() - t_i\n",
        "                    err[i,j], std[i,j] = model.test(X_data1['test'], Y_data['test'], GSO)\n",
        "                    params = arch.n_params if hasattr(arch, 'n_params') else None\n",
        "\n",
        "\n",
        "                elif exp['arc_p']['arch'] == DAGNN:\n",
        "                    X_data1, Y_data1 = DAGNN_model(Adj, X_data, Y_data)\n",
        "                    GSO = utils.select_GSO(arc_p, GSOs, sel_GSOs, W, Adj)\n",
        "                    K = GSO.shape[0] if isinstance(GSO, torch.Tensor) and len(GSO.shape) == 3 else 0\n",
        "                    arch = utils.instantiate_arch(arc_p, K)\n",
        "                    model = Model(arch, device=device)\n",
        "                    t_i = time.time()\n",
        "                    model.fit(X_data1, Y_data1, GSO, mod_p['lr'], mod_p['epochs'], mod_p['bs'], mod_p['wd'], patience=mod_p['pat'])\n",
        "                    t_e = time.time() - t_i\n",
        "                    err[i,j], std[i,j] = model.test(X_data1['test'], Y_data1['test'], GSO)\n",
        "                    params = arch.n_params if hasattr(arch, 'n_params') else None\n",
        "                else:\n",
        "                    # Fit and test nonlinear models\n",
        "                    GSO = utils.select_GSO(arc_p, GSOs, sel_GSOs, W, Adj)\n",
        "                    K = GSO.shape[0] if isinstance(GSO, torch.Tensor) and len(GSO.shape) == 3 else 0\n",
        "                    arch = utils.instantiate_arch(arc_p, K)\n",
        "                    model = Model(arch, device=device)\n",
        "                    t_i = time.time()\n",
        "                    model.fit(X_data, Y_data, GSO, mod_p['lr'], mod_p['epochs'], mod_p['bs'], mod_p['wd'], patience=mod_p['pat'])\n",
        "                    t_e = time.time() - t_i\n",
        "                    err[i,j], std[i,j] = model.test(X_data['test'], Y_data['test'], GSO)\n",
        "                    params = arch.n_params if hasattr(arch, 'n_params') else None\n",
        "\n",
        "                times[i,j] = t_e\n",
        "\n",
        "                # params = arch.n_params if hasattr(arch, 'n_params') else None\n",
        "\n",
        "                # Progress\n",
        "                pbar.update(1)\n",
        "                if verb:\n",
        "                    print(f'-{i}. {exp[\"leg\"]}: err: {err[i,j]:.3f} | std: {std[i,j]:.3f}  |' +\n",
        "                          f' time: {times[i,j]:.1f} | n_params: {params}')\n",
        "\n",
        "    total_t = (time.time() - t_begin)/60\n",
        "    print(f'----- Ellapsed time: {total_t:.2f} minutes -----')\n",
        "    return err, std, times\n",
        "\n",
        "\n",
        "\n",
        "Exps = [\n",
        "    # Our Models\n",
        "\n",
        "    # {'arc_p': {'arch': ParallelMLPSum , 'GSO': 'GSOs', 'n_inputs': 20, 'input_dim': 20, 'hidden_dims': [32], 'output_dim': 20}, 'leg': 'ParallelMLPSum - 1 layer, hid_dim:32'},\n",
        "    # {'arc_p': {'arch': SharedMLPSum , 'GSO': 'GSOs', 'n_inputs': 20, 'input_dim': 20, 'hidden_dims': [32], 'output_dim': 20}, 'leg': 'SharedMLPSum - 1 layer, hid_dim:32'},\n",
        "    # {'arc_p': {'arch': SMLP2 , 'GSO': 'GSOs', 'in_dim': 1, 'hid_dim': [128], 'out_dim': 1, 'bias' : True }, 'leg': 'SMLP2-128'},\n",
        "    # {'arc_p': {'arch': SMLP , 'GSO': 'GSOs', 'in_dim': 1, 'hid_dim': [128], 'out_dim': 1, 'bias' : True }, 'leg': 'SMLP-128'},\n",
        "\n",
        "    # {'arc_p': {'arch': SMLP , 'GSO': 'GSOs', 'in_dim': 1, 'hid_dim': [256], 'out_dim': 1, 'bias' : True }, 'leg': 'SMLP-256'},\n",
        "    # {'arc_p': {'arch': SMLP , 'GSO': 'GSOs', 'in_dim': 1, 'hid_dim': [64], 'out_dim': 1, 'bias' : True }, 'leg': 'SMLP-64'},\n",
        "\n",
        "\n",
        "    # {'arc_p': {'arch': ParallelMLPSum , 'GSO': 'GSOs', 'n_inputs': 20, 'input_dim': 20, 'hidden_dims': [64], 'output_dim': 20}, 'leg': 'ParallelMLPSum - 1 layer, hid_dim:64'},\n",
        "    # {'arc_p': {'arch': SharedMLPSum , 'GSO': 'GSOs', 'n_inputs': 20, 'input_dim': 20, 'hidden_dims': [64], 'output_dim': 20}, 'leg': 'SharedMLPSum - 1 layer, hid_dim:64'},\n",
        "\n",
        "\n",
        "    {'arc_p': {'arch': FB_DAGConv, 'GSO': 'GSOs'}, 'leg': 'DCN'},\n",
        "\n",
        "    {'arc_p': {'arch': FB_DAGConv, 'GSO': 'rnd_GSOs', 'n_gsos': 15}, 'leg': 'DCN-15'},\n",
        "    {'arc_p': {'arch': SMLP , 'GSO': 'GSOs', 'in_dim': 1, 'hid_dim': [128], 'out_dim': 1, 'bias' : True }, 'leg': 'SMLP-128'},\n",
        "\n",
        "    # {'arc_p': {'arch': FB_DAGConv, 'GSO': 'rnd_GSOs', 'n_gsos': 10}, 'leg': 'DCN-10'},\n",
        "    # {'arc_p': {'arch': FB_DAGConv, 'GSO': 'rnd_GSOs', 'n_gsos': 5}, 'leg': 'DCN-5'},\n",
        "\n",
        "    # {'arc_p': {'arch': DAGConv, 'GSO': 'GSOs'}, 'leg': 'DAGConv'},\n",
        "    # {'arc_p': {'arch': DAGConv, 'GSO': 'rnd_GSOs', 'n_gsos': 15}, 'leg': 'DAGConv-15'},\n",
        "    # {'arc_p': {'arch': DAGConv, 'GSO': 'rnd_GSOs', 'n_gsos': 10}, 'leg': 'DAGConv-10'},\n",
        "    # {'arc_p': {'arch': DAGConv, 'GSO': 'rnd_GSOs', 'n_gsos': 5}, 'leg': 'DAGConv-5'},\n",
        "\n",
        "\n",
        "    {'arc_p': {'arch': FB_DAGConv, 'GSO': 'GSOs', 'transp': True}, 'leg': 'DCN-T'},\n",
        "\n",
        "\n",
        "    # {'arc_p': {'arch': FB_DAGConv, 'GSO': 'rnd_GSOs', 'n_gsos': 15, 'transp': True}, 'leg': 'DCN-15-T'},\n",
        "    # {'arc_p': {'arch': FB_DAGConv, 'GSO': 'rnd_GSOs', 'n_gsos': 10, 'transp': True}, 'leg': 'DCN-10-T'},\n",
        "\n",
        "    # {'arc_p': {'arch': DAGConv, 'GSO': 'GSOs', 'transp': True}, 'leg': 'DAGConv-T'},\n",
        "    # {'arc_p': {'arch': DAGConv, 'GSO': 'rnd_GSOs', 'n_gsos': 15, 'transp': True}, 'leg': 'DAGConv-15-T'},\n",
        "    # {'arc_p': {'arch': ADCN, 'GSO': 'GSOs', 'args': {'mlp_layers': 4}}, 'leg': 'ADCN-4'},\n",
        "\n",
        "    # {'arc_p': {'arch': ADCN, 'GSO': 'GSOs', 'transp': True, 'args': {'mlp_layers': 4}}, 'leg': 'ADCN-4-T'},\n",
        "    # {'arc_p': {'arch': ADCN, 'GSO': 'GSOs', 'transp': True, 'args': {'mlp_layers': 5}}, 'leg': 'ADCN-5-T'},\n",
        "\n",
        "\n",
        "    {'arc_p': {'arch': LinDAGRegModel, 'GSO': 'GSOs'}, 'leg': 'Linear'},\n",
        "    {'arc_p': {'arch': LinDAGRegModel, 'GSO': 'rnd_GSOs', 'n_gsos': 15}, 'leg': 'Linear-15'},\n",
        "\n",
        "\n",
        "\n",
        "    # {'arc_p': {'arch': LinDAGRegModel, 'GSO': 'rnd_GSOs', 'n_gsos': 10}, 'leg': 'Linear-10'},\n",
        "    # {'arc_p': {'arch': LinDAGRegModel, 'GSO': 'GSOs', 'transp': True}, 'leg': 'Linear-T'},\n",
        "    # {'arc_p': {'arch': LinDAGRegModel, 'GSO': 'rnd_GSOs', 'n_gsos': 15, 'transp': True}, 'leg': 'Linear-15-T'},\n",
        "    # {'arc_p': {'arch': LinDAGRegModel, 'GSO': 'rnd_GSOs', 'n_gsos': 10, 'transp': True}, 'leg': 'Linear-10-T'},\n",
        "\n",
        "\n",
        "    # {'arc_p': {'arch': ADCN, 'GSO': 'GSOs', 'args': {'mlp_layers': 4}}, 'leg': 'ADCN-4-2'},\n",
        "    # {'arc_p': {'arch': ADCN, 'GSO': 'GSOs', 'args': {'mlp_layers': 5}}, 'leg': 'ADCN-5-2'},\n",
        "    # {'arc_p': {'arch': ADCN, 'GSO': 'GSOs', 'args': {'mlp_layers': 4, 'n_layers': 4}}, 'leg': 'ADCN-4-4'},\n",
        "    # {'arc_p': {'arch': ADCN, 'GSO': 'GSOs', 'args': {'mlp_layers': 5, 'n_layers': 4}}, 'leg': 'ADCN-5-4'},\n",
        "\n",
        "\n",
        "    {'arc_p': {'arch': DAGNN, 'GSO': 'GSOs', 'emb_dim': 1, 'hidden_dim':128, 'out_dim': 128, 'max_n': 20,'nvt':1 ,\n",
        "               'START_TYPE': 0, 'END_TYPE': 1, 'hs':128, 'nz': 56, 'agg': \"attn_h\", 'num_layers':2, 'bidirectional': True, 'out_wx': False, 'out_pool_all': False,\n",
        "               'out_pool': P_MAX, 'dropout': 0.2, 'num_nodes': 20}, 'leg': 'DAGNN'},\n",
        "\n",
        "    {'arc_p': {'arch': DVAE, 'GSO': 'GSOs','max_n':20, 'nvt':1, 'START_TYPE':0, 'END_TYPE':1, 'hs':128,'nz':20, 'bidirectional': True, 'vid': True}, 'leg': 'DVAE'},\n",
        "\n",
        "\n",
        "    {'arc_p': {'arch': GraphSAGE, 'GSO': 'A-dgl', 'args': {'aggregator': 'mean'}}, 'leg': 'GraphSAGE-A'},\n",
        "    {'arc_p': {'arch': GIN, 'GSO': 'A-dgl', 'args': {'aggregator': 'sum'}}, 'leg': 'GIN-A'},\n",
        "    {'arc_p': {'arch': GIN, 'GSO': 'A-dgl', 'args': {'aggregator': 'sum', 'mlp_layers': 4}}, 'leg': 'GIN-A-4'},\n",
        "    {'arc_p': {'arch': MLP, 'GSO': None}, 'leg': 'MLP'},\n",
        "\n",
        "\n",
        "    # {'arc_p': {'arch': MLP, 'GSO': None, 'args': {'n_layers': 4}}, 'leg': 'MLP-4'},\n",
        "\n",
        "\n",
        "\n",
        "    {'arc_p': {'arch': MyGCNN, 'GSO': 'A'}, 'leg': 'GNN-A'},\n",
        "    {'arc_p': {'arch': FB_DAGConv, 'GSO': 'A_pows', 'K': 2, 'transp': False}, 'leg': 'FB-GCNN-2'},\n",
        "    # {'arc_p': {'arch': FB_DAGConv, 'GSO': 'A_pows', 'K': 3, 'transp': False}, 'leg': 'FB-GCNN-3'},\n",
        "    # {'arc_p': {'arch': FB_DAGConv, 'GSO': 'A_pows', 'K': 4, 'transp': False}, 'leg': 'FB-GCNN-4'},\n",
        "    {'arc_p': {'arch': GAT, 'GSO': 'A-dgl', 'args': {'num_heads': 2, 'hid_dim': 16, 'gat_params': {'attn_drop': 0}}},\n",
        "     'leg': 'GAT'},\n",
        "\n",
        "    ]\n",
        "\n",
        "mod_p_init = default_mod_p.copy()\n",
        "mod_p_init['pat'] = 50\n",
        "verb = True\n",
        "err_chloride, std_chloride, times_chloride = run_exp(data_p, default_arch_args, mod_p_init, Exps, verb=verb)"
      ],
      "metadata": {
        "id": "AnsBBav7WQh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "# import dgl\n",
        "import torch\n",
        "from torch import nn\n",
        "import networkx as nx\n",
        "from tqdm.auto import tqdm\n",
        "import sys\n",
        "import os\n",
        "import pickle\n",
        "from numpy import linalg as la\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "\n",
        "\n",
        "import src.dag_utils as dagu\n",
        "import src.utils as utils\n",
        "from src.arch import DAGConv, FB_DAGConv, SF_DAGConv, ADCN , ParallelMLPSum, SharedMLPSum, SMLP\n",
        "from src.models import Model, LinDAGRegModel\n",
        "from src.baselines_archs import GAT, MLP, MyGCNN, GraphSAGE, GIN\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"dgl\")\n",
        "import os\n",
        "import igraph as ig\n",
        "from src.utils_Dagnn import *\n",
        "from src.DAGNN import DAGNN, DVAE\n",
        "\n",
        "\n",
        "\n",
        "# Ser random seed\n",
        "SEED = 10\n",
        "PATH = 'results/diffusion/'\n",
        "SAVE = True\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "# dgl.random.seed(SEED)\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "# torch.cuda.set_per_process_memory_fraction(.5, device=device)\n",
        "\n",
        "\n",
        "\n",
        "M = 343\n",
        "\n",
        "data_p = {\n",
        "    'n_tries': 25,  #25,\n",
        "\n",
        "    ## Graph parameters\n",
        "    'p': 0.2,  # .2                  # Edge prob in Erdos-Renyi DAG\n",
        "    'N': 20,                    # Number of nodes\n",
        "\n",
        "    ## Signal parameters\n",
        "    'M': M,                   # Number of observed signals\n",
        "    'M_train': int(0.7 * M),  # Samples selected for training\n",
        "    'M_val': int(0.2 * M),    # Samples selected for validation\n",
        "    'M_test': int(0.1 * M),   # Samples selected for test\n",
        "    'src_t': 'constant',          # 'random' or 'constant'\n",
        "    'max_src_node': 15, #25,           # Maximum index of nodes allowed to be sources\n",
        "    'n_sources': 14,             # Maximum Number of source nodes\n",
        "    'n_p_x': .05,\n",
        "    'n_p_y': .05,                 # Normalized noise power\n",
        "    'max_GSO': 20,              # Maximum index of GSOs involved in the diffusion\n",
        "    'min_GSO': 0,               # Minimum index of GSOs involved in the diffusion\n",
        "    'n_GSOs': 20,            # Number of GSOs\n",
        "    'concentration': 'silicon.npy'\n",
        "}\n",
        "\n",
        "default_mod_p = {\n",
        "    'bs': 25,           # Size of the batch\n",
        "    'lr': 5e-4,         # Learning rate\n",
        "    'epochs': 100,  #50,       # Number of training epochs\n",
        "    'pat': 25,  # 15        # Number of non-decreasing epoch to stop training\n",
        "    'wd': 1e-4,         # Weight decay\n",
        "}\n",
        "\n",
        "default_arch_args = {\n",
        "    'in_dim': 1,        # Input dimension\n",
        "    'hid_dim': 32,     # Hidden dimension\n",
        "    'out_dim': 1,       # Output dimension\n",
        "    'n_layers': 2,#2,  # 3 also works well          # Number of layers\n",
        "    'l_act': None,\n",
        "    'bias': True,\n",
        "}\n",
        "\n",
        "\n",
        "def add_noise(signal, n_p):\n",
        "    shape = signal.shape\n",
        "\n",
        "    M = shape[0]\n",
        "    N = shape[1]\n",
        "\n",
        "    if n_p <= 0:\n",
        "        return signal\n",
        "\n",
        "    signal_norm = torch.norm(signal, p=2, dim=1, keepdim=True)\n",
        "    signal_norm[signal_norm == 0] = 1\n",
        "    noise = torch.randn(M, N, 1, device=signal.device)\n",
        "    noise_norm = torch.norm(noise, p=2, dim=1, keepdim=True)\n",
        "    noise = noise * signal_norm * torch.sqrt(torch.tensor(n_p)) / noise_norm\n",
        "\n",
        "    return signal + noise\n",
        "\n",
        "\n",
        "\n",
        "def get_real_data(d_dat_p ,get_Psi=False):\n",
        "\n",
        "    nodes = ['CH','CL','CN','CU','EN','EV','LE','LO','OC','PA','RA','TH','TM','WI','KE','TN','TS','TW','TSO','TR']\n",
        "\n",
        "    Adj = np.load('/content/drive/MyDrive/Thames/'+'adj_matrix.npy')\n",
        "\n",
        "    W = la.inv(np.eye(d_dat_p['N']) - Adj)\n",
        "    W_inf = la.inv(W)\n",
        "    dag = nx.from_numpy_array(Adj.T, create_using=nx.DiGraph())\n",
        "    if get_Psi:\n",
        "        Psi = np.array([dagu.compute_Dq(dag, i, d_dat_p['N']) for i in range(d_dat_p['N'])]).T\n",
        "        GSOs = np.array([(W * Psi[:,i]) @ W_inf for i in range(d_dat_p['N'])])\n",
        "        return Adj, W, GSOs, Psi\n",
        "    GSOs = np.array([(W * dagu.compute_Dq(dag, i, d_dat_p['N'])) @ W_inf for i in range(d_dat_p['N'])])\n",
        "\n",
        "    return Adj, W, GSOs\n",
        "\n",
        "\n",
        "def is_upper_triangular(adj_matrix):\n",
        "    return np.allclose(adj_matrix, np.triu(adj_matrix))\n",
        "\n",
        "def randomize_dag_adj(A_true, alpha):\n",
        "    num_nodes = A_true.shape[0]\n",
        "\n",
        "    while True:\n",
        "        A_rand = np.triu(A_true.copy())\n",
        "\n",
        "        total_possible_edges = (num_nodes * (num_nodes - 1)) // 2\n",
        "        num_modifications = int(alpha * total_possible_edges)\n",
        "\n",
        "        possible_edges = [(i, j) for i in range(num_nodes) for j in range(i+1, num_nodes)]\n",
        "\n",
        "        modifications = 0\n",
        "        while modifications < num_modifications:\n",
        "            # Randomly choose to add or remove an edge\n",
        "            action = random.choice(['add', 'remove'])\n",
        "            i, j = random.choice(possible_edges)\n",
        "\n",
        "            if action == 'add' and A_rand[i, j] == 0:\n",
        "                A_rand[i, j] = 1\n",
        "                modifications += 1\n",
        "            elif action == 'remove' and A_rand[i, j] == 1:\n",
        "                A_rand[i, j] = 0\n",
        "                modifications += 1\n",
        "\n",
        "        if is_upper_triangular(A_rand):\n",
        "            return A_rand\n",
        "\n",
        "\n",
        "def Thames(d_p,GSOs, pollutant=None):\n",
        "\n",
        "    range_GSO = np.arange(d_p['min_GSO'], d_p['max_GSO'])\n",
        "    gsos_idx = np.random.choice(range_GSO, size=d_p['n_GSOs'], replace=False)\n",
        "    sel_GSOs = GSOs[gsos_idx]\n",
        "    t1 = np.load('/content/drive/MyDrive/Thames/' + pollutant)\n",
        "    Yn_t = torch.tensor(t1, dtype = torch.float32)\n",
        "\n",
        "    Xn_t = Yn_t.clone()\n",
        "    Xn_t[:,-6:,:]  = 0\n",
        "\n",
        "    Xn_t = add_noise(Xn_t, 0.00)\n",
        "    Yn_t = add_noise(Yn_t, 0.00)\n",
        "\n",
        "    X_data = {'train': Xn_t[:d_p['M_train']], 'val': Xn_t[d_p['M_train']:-d_p['M_test']], 'test': Xn_t[-d_p['M_test']:]}\n",
        "    Y_data = {'train': Yn_t[:d_p['M_train']], 'val': Yn_t[d_p['M_train']:-d_p['M_test']], 'test': Yn_t[-d_p['M_test']:]}\n",
        "\n",
        "    return X_data, Y_data, sel_GSOs, gsos_idx\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_signals(d_p, GSOs):\n",
        "    range_GSO = np.arange(d_p['min_GSO'], d_p['max_GSO'])\n",
        "    gsos_idx = np.random.choice(range_GSO, size=d_p['n_GSOs'], replace=False)\n",
        "    sel_GSOs = GSOs[gsos_idx]\n",
        "    Yn_t, X_t, Y_t = dagu.create_diff_data(d_p['M'], sel_GSOs, d_p['max_src_node'], d_p['n_p_x'], d_p['n_p_y'],\n",
        "                                           d_p['n_sources'], src_t=d_p['src_t'], torch_tensor=True, verb=False)\n",
        "\n",
        "\n",
        "\n",
        "    X_data = {'train': X_t[:d_p['M_train']], 'val': X_t[d_p['M_train']:-d_p['M_test']], 'test': X_t[-d_p['M_test']:]}\n",
        "    Y_data = {'train': Yn_t[:d_p['M_train']], 'val': Yn_t[d_p['M_train']:-d_p['M_test']],\n",
        "              'test': Y_t[-d_p['M_test']:]}\n",
        "\n",
        "    return X_data, Y_data, sel_GSOs, gsos_idx\n",
        "\n",
        "\n",
        "def run_exp(d_p, d_arc_args, d_mod_p, exps, verb=True):\n",
        "    # Create error variables\n",
        "    print(d_p['concentration'])\n",
        "    err = np.zeros((d_p['n_tries'], len(exps)))\n",
        "    std = np.zeros((d_p['n_tries'], len(exps)))\n",
        "    times = np.zeros((d_p['n_tries'], len(exps)))\n",
        "\n",
        "    t_begin = time.time()\n",
        "    # for i in range(d_p['n_tries']):\n",
        "    with tqdm(total=d_p['n_tries']*len(exps), disable=False) as pbar:\n",
        "        for i in range(d_p['n_tries']):\n",
        "            Adj, W, GSOs, Psi = get_real_data(d_p, get_Psi=True)\n",
        "\n",
        "            X_data, Y_data, sel_GSOs, sel_GSOs_idx = Thames(d_p, GSOs, d_p['concentration'])\n",
        "\n",
        "            for j, exp in enumerate(exps):\n",
        "                arc_p = {**exp['arc_p']}\n",
        "\n",
        "                arc_p['args'] = {**d_arc_args, **arc_p['args']} if 'args' in arc_p.keys() else {**d_arc_args}\n",
        "                mod_p = {**d_mod_p, **exp['mod_p']} if 'mod_p' in exp.keys() else d_mod_p\n",
        "\n",
        "                if exp['arc_p']['arch'] == LinDAGRegModel:\n",
        "                    # Fit and test linear model\n",
        "                    if 'transp' in arc_p.keys() and arc_p['transp']:\n",
        "                        dag_T = nx.from_numpy_array(Adj, create_using=nx.DiGraph())\n",
        "                        Psi = np.array([dagu.compute_Dq(dag_T, i, d_p['N']) for i in range(d_p['N'])]).T\n",
        "                        arc_p['transp'] = False\n",
        "\n",
        "                    Psi_sel = utils.select_GSO(arc_p, Psi.T, Psi[:,sel_GSOs_idx].T, W, Adj, sel_GSOs_idx).numpy().T\n",
        "                    lin_model = LinDAGRegModel(W, Psi_sel)\n",
        "\n",
        "                    t_i = time.time()\n",
        "                    lin_model.fit(X_data['train'], Y_data['train'])\n",
        "                    t_e = time.time() - t_i\n",
        "                    err[i,j], std[i,j] = lin_model.test(X_data['test'], Y_data['test'])\n",
        "                    params = lin_model.n_params if hasattr(lin_model, 'n_params') else None\n",
        "\n",
        "\n",
        "                elif exp['arc_p']['arch'] == DVAE:\n",
        "                    X_data1 = DVAE_exp(Adj, X_data)\n",
        "                    GSO = utils.select_GSO(arc_p, GSOs, sel_GSOs, W, Adj)\n",
        "                    K = GSO.shape[0] if isinstance(GSO, torch.Tensor) and len(GSO.shape) == 3 else 0\n",
        "                    arch = utils.instantiate_arch(arc_p, K)\n",
        "                    model = Model(arch, device=device)\n",
        "\n",
        "                    t_i = time.time()\n",
        "                    model.fit(X_data1, Y_data, GSO, mod_p['lr'], mod_p['epochs'], mod_p['bs'], mod_p['wd'], patience=mod_p['pat'])\n",
        "                    t_e = time.time() - t_i\n",
        "                    err[i,j], std[i,j] = model.test(X_data1['test'], Y_data['test'], GSO)\n",
        "                    params = arch.n_params if hasattr(arch, 'n_params') else None\n",
        "\n",
        "\n",
        "                elif exp['arc_p']['arch'] == DAGNN:\n",
        "                    X_data1, Y_data1 = DAGNN_model(Adj, X_data, Y_data)\n",
        "                    GSO = utils.select_GSO(arc_p, GSOs, sel_GSOs, W, Adj)\n",
        "                    K = GSO.shape[0] if isinstance(GSO, torch.Tensor) and len(GSO.shape) == 3 else 0\n",
        "                    arch = utils.instantiate_arch(arc_p, K)\n",
        "                    model = Model(arch, device=device)\n",
        "                    t_i = time.time()\n",
        "                    model.fit(X_data1, Y_data1, GSO, mod_p['lr'], mod_p['epochs'], mod_p['bs'], mod_p['wd'], patience=mod_p['pat'])\n",
        "                    t_e = time.time() - t_i\n",
        "                    err[i,j], std[i,j] = model.test(X_data1['test'], Y_data1['test'], GSO)\n",
        "                    params = arch.n_params if hasattr(arch, 'n_params') else None\n",
        "                else:\n",
        "                    # Fit and test nonlinear models\n",
        "                    GSO = utils.select_GSO(arc_p, GSOs, sel_GSOs, W, Adj)\n",
        "                    K = GSO.shape[0] if isinstance(GSO, torch.Tensor) and len(GSO.shape) == 3 else 0\n",
        "                    arch = utils.instantiate_arch(arc_p, K)\n",
        "                    model = Model(arch, device=device)\n",
        "                    t_i = time.time()\n",
        "                    model.fit(X_data, Y_data, GSO, mod_p['lr'], mod_p['epochs'], mod_p['bs'], mod_p['wd'], patience=mod_p['pat'])\n",
        "                    t_e = time.time() - t_i\n",
        "                    err[i,j], std[i,j] = model.test(X_data['test'], Y_data['test'], GSO)\n",
        "                    params = arch.n_params if hasattr(arch, 'n_params') else None\n",
        "\n",
        "                times[i,j] = t_e\n",
        "\n",
        "                # params = arch.n_params if hasattr(arch, 'n_params') else None\n",
        "\n",
        "                # Progress\n",
        "                pbar.update(1)\n",
        "                if verb:\n",
        "                    print(f'-{i}. {exp[\"leg\"]}: err: {err[i,j]:.3f} | std: {std[i,j]:.3f}  |' +\n",
        "                          f' time: {times[i,j]:.1f} | n_params: {params}')\n",
        "\n",
        "    total_t = (time.time() - t_begin)/60\n",
        "    print(f'----- Ellapsed time: {total_t:.2f} minutes -----')\n",
        "    return err, std, times\n",
        "\n",
        "\n",
        "\n",
        "Exps = [\n",
        "    # Our Models\n",
        "\n",
        "    # {'arc_p': {'arch': ParallelMLPSum , 'GSO': 'GSOs', 'n_inputs': 20, 'input_dim': 20, 'hidden_dims': [32], 'output_dim': 20}, 'leg': 'ParallelMLPSum - 1 layer, hid_dim:32'},\n",
        "    # {'arc_p': {'arch': SharedMLPSum , 'GSO': 'GSOs', 'n_inputs': 20, 'input_dim': 20, 'hidden_dims': [32], 'output_dim': 20}, 'leg': 'SharedMLPSum - 1 layer, hid_dim:32'},\n",
        "    # {'arc_p': {'arch': SMLP2 , 'GSO': 'GSOs', 'in_dim': 1, 'hid_dim': [128], 'out_dim': 1, 'bias' : True }, 'leg': 'SMLP2-128'},\n",
        "    # {'arc_p': {'arch': SMLP , 'GSO': 'GSOs', 'in_dim': 1, 'hid_dim': [128], 'out_dim': 1, 'bias' : True }, 'leg': 'SMLP-128'},\n",
        "\n",
        "    # {'arc_p': {'arch': SMLP , 'GSO': 'GSOs', 'in_dim': 1, 'hid_dim': [256], 'out_dim': 1, 'bias' : True }, 'leg': 'SMLP-256'},\n",
        "    # {'arc_p': {'arch': SMLP , 'GSO': 'GSOs', 'in_dim': 1, 'hid_dim': [64], 'out_dim': 1, 'bias' : True }, 'leg': 'SMLP-64'},\n",
        "\n",
        "\n",
        "    # {'arc_p': {'arch': ParallelMLPSum , 'GSO': 'GSOs', 'n_inputs': 20, 'input_dim': 20, 'hidden_dims': [64], 'output_dim': 20}, 'leg': 'ParallelMLPSum - 1 layer, hid_dim:64'},\n",
        "    # {'arc_p': {'arch': SharedMLPSum , 'GSO': 'GSOs', 'n_inputs': 20, 'input_dim': 20, 'hidden_dims': [64], 'output_dim': 20}, 'leg': 'SharedMLPSum - 1 layer, hid_dim:64'},\n",
        "\n",
        "\n",
        "    {'arc_p': {'arch': FB_DAGConv, 'GSO': 'GSOs'}, 'leg': 'DCN'},\n",
        "\n",
        "    {'arc_p': {'arch': FB_DAGConv, 'GSO': 'rnd_GSOs', 'n_gsos': 15}, 'leg': 'DCN-15'},\n",
        "    {'arc_p': {'arch': SMLP , 'GSO': 'GSOs', 'in_dim': 1, 'hid_dim': [128], 'out_dim': 1, 'bias' : True }, 'leg': 'SMLP-128'},\n",
        "\n",
        "    # {'arc_p': {'arch': FB_DAGConv, 'GSO': 'rnd_GSOs', 'n_gsos': 10}, 'leg': 'DCN-10'},\n",
        "    # {'arc_p': {'arch': FB_DAGConv, 'GSO': 'rnd_GSOs', 'n_gsos': 5}, 'leg': 'DCN-5'},\n",
        "\n",
        "    # {'arc_p': {'arch': DAGConv, 'GSO': 'GSOs'}, 'leg': 'DAGConv'},\n",
        "    # {'arc_p': {'arch': DAGConv, 'GSO': 'rnd_GSOs', 'n_gsos': 15}, 'leg': 'DAGConv-15'},\n",
        "    # {'arc_p': {'arch': DAGConv, 'GSO': 'rnd_GSOs', 'n_gsos': 10}, 'leg': 'DAGConv-10'},\n",
        "    # {'arc_p': {'arch': DAGConv, 'GSO': 'rnd_GSOs', 'n_gsos': 5}, 'leg': 'DAGConv-5'},\n",
        "\n",
        "\n",
        "    {'arc_p': {'arch': FB_DAGConv, 'GSO': 'GSOs', 'transp': True}, 'leg': 'DCN-T'},\n",
        "\n",
        "\n",
        "    # {'arc_p': {'arch': FB_DAGConv, 'GSO': 'rnd_GSOs', 'n_gsos': 15, 'transp': True}, 'leg': 'DCN-15-T'},\n",
        "    # {'arc_p': {'arch': FB_DAGConv, 'GSO': 'rnd_GSOs', 'n_gsos': 10, 'transp': True}, 'leg': 'DCN-10-T'},\n",
        "\n",
        "    # {'arc_p': {'arch': DAGConv, 'GSO': 'GSOs', 'transp': True}, 'leg': 'DAGConv-T'},\n",
        "    # {'arc_p': {'arch': DAGConv, 'GSO': 'rnd_GSOs', 'n_gsos': 15, 'transp': True}, 'leg': 'DAGConv-15-T'},\n",
        "    # {'arc_p': {'arch': ADCN, 'GSO': 'GSOs', 'args': {'mlp_layers': 4}}, 'leg': 'ADCN-4'},\n",
        "\n",
        "    # {'arc_p': {'arch': ADCN, 'GSO': 'GSOs', 'transp': True, 'args': {'mlp_layers': 4}}, 'leg': 'ADCN-4-T'},\n",
        "    # {'arc_p': {'arch': ADCN, 'GSO': 'GSOs', 'transp': True, 'args': {'mlp_layers': 5}}, 'leg': 'ADCN-5-T'},\n",
        "\n",
        "\n",
        "    {'arc_p': {'arch': LinDAGRegModel, 'GSO': 'GSOs'}, 'leg': 'Linear'},\n",
        "    {'arc_p': {'arch': LinDAGRegModel, 'GSO': 'rnd_GSOs', 'n_gsos': 15}, 'leg': 'Linear-15'},\n",
        "\n",
        "\n",
        "\n",
        "    # {'arc_p': {'arch': LinDAGRegModel, 'GSO': 'rnd_GSOs', 'n_gsos': 10}, 'leg': 'Linear-10'},\n",
        "    # {'arc_p': {'arch': LinDAGRegModel, 'GSO': 'GSOs', 'transp': True}, 'leg': 'Linear-T'},\n",
        "    # {'arc_p': {'arch': LinDAGRegModel, 'GSO': 'rnd_GSOs', 'n_gsos': 15, 'transp': True}, 'leg': 'Linear-15-T'},\n",
        "    # {'arc_p': {'arch': LinDAGRegModel, 'GSO': 'rnd_GSOs', 'n_gsos': 10, 'transp': True}, 'leg': 'Linear-10-T'},\n",
        "\n",
        "\n",
        "    # {'arc_p': {'arch': ADCN, 'GSO': 'GSOs', 'args': {'mlp_layers': 4}}, 'leg': 'ADCN-4-2'},\n",
        "    # {'arc_p': {'arch': ADCN, 'GSO': 'GSOs', 'args': {'mlp_layers': 5}}, 'leg': 'ADCN-5-2'},\n",
        "    # {'arc_p': {'arch': ADCN, 'GSO': 'GSOs', 'args': {'mlp_layers': 4, 'n_layers': 4}}, 'leg': 'ADCN-4-4'},\n",
        "    # {'arc_p': {'arch': ADCN, 'GSO': 'GSOs', 'args': {'mlp_layers': 5, 'n_layers': 4}}, 'leg': 'ADCN-5-4'},\n",
        "\n",
        "\n",
        "    {'arc_p': {'arch': DAGNN, 'GSO': 'GSOs', 'emb_dim': 1, 'hidden_dim':128, 'out_dim': 128, 'max_n': 20,'nvt':1 ,\n",
        "               'START_TYPE': 0, 'END_TYPE': 1, 'hs':128, 'nz': 56, 'agg': \"attn_h\", 'num_layers':2, 'bidirectional': True, 'out_wx': False, 'out_pool_all': False,\n",
        "               'out_pool': P_MAX, 'dropout': 0.2, 'num_nodes': 20}, 'leg': 'DAGNN'},\n",
        "\n",
        "    {'arc_p': {'arch': DVAE, 'GSO': 'GSOs','max_n':20, 'nvt':1, 'START_TYPE':0, 'END_TYPE':1, 'hs':128,'nz':20, 'bidirectional': True, 'vid': True}, 'leg': 'DVAE'},\n",
        "\n",
        "\n",
        "    {'arc_p': {'arch': GraphSAGE, 'GSO': 'A-dgl', 'args': {'aggregator': 'mean'}}, 'leg': 'GraphSAGE-A'},\n",
        "    {'arc_p': {'arch': GIN, 'GSO': 'A-dgl', 'args': {'aggregator': 'sum'}}, 'leg': 'GIN-A'},\n",
        "    {'arc_p': {'arch': GIN, 'GSO': 'A-dgl', 'args': {'aggregator': 'sum', 'mlp_layers': 4}}, 'leg': 'GIN-A-4'},\n",
        "    {'arc_p': {'arch': MLP, 'GSO': None}, 'leg': 'MLP'},\n",
        "\n",
        "\n",
        "    # {'arc_p': {'arch': MLP, 'GSO': None, 'args': {'n_layers': 4}}, 'leg': 'MLP-4'},\n",
        "\n",
        "\n",
        "\n",
        "    {'arc_p': {'arch': MyGCNN, 'GSO': 'A'}, 'leg': 'GNN-A'},\n",
        "    {'arc_p': {'arch': FB_DAGConv, 'GSO': 'A_pows', 'K': 2, 'transp': False}, 'leg': 'FB-GCNN-2'},\n",
        "    # {'arc_p': {'arch': FB_DAGConv, 'GSO': 'A_pows', 'K': 3, 'transp': False}, 'leg': 'FB-GCNN-3'},\n",
        "    # {'arc_p': {'arch': FB_DAGConv, 'GSO': 'A_pows', 'K': 4, 'transp': False}, 'leg': 'FB-GCNN-4'},\n",
        "    {'arc_p': {'arch': GAT, 'GSO': 'A-dgl', 'args': {'num_heads': 2, 'hid_dim': 16, 'gat_params': {'attn_drop': 0}}},\n",
        "     'leg': 'GAT'},\n",
        "\n",
        "    ]\n",
        "\n",
        "mod_p_init = default_mod_p.copy()\n",
        "mod_p_init['pat'] = 50\n",
        "verb = True\n",
        "err_silicon, std_silicon, times_silicon = run_exp(data_p, default_arch_args, mod_p_init, Exps, verb=verb)"
      ],
      "metadata": {
        "id": "NnoH82sOWTZW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}